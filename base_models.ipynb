{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/workspace/s3/hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dst = load_from_disk(\"../s3/hf/datasets/python_code_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def predict_with_model(model, tokenizer, dataset, batch_size: int, max_length: int = 2048, max_gen_tokens=100):\n",
    "    tokenized = dataset.map(lambda x: tokenizer(x[\"text\"], padding=\"max_length\", \n",
    "                                                add_special_tokens=False, max_length=max_length,\n",
    "                                                truncation=True), batched=True, num_proc=4)\n",
    "\n",
    "    predictions = []\n",
    "    for batch_start in tqdm(range(0, len(dataset), batch_size)):\n",
    "        tokenized_batch = tokenized[batch_start:batch_start+batch_size]\n",
    "        \n",
    "        inputs = torch.tensor(tokenized_batch[\"input_ids\"]).to(\"cuda\")\n",
    "        attention_mask = torch.tensor(tokenized_batch[\"attention_mask\"]).to(\"cuda\")\n",
    "        generated_tokens = model.generate(inputs=inputs,\n",
    "                                attention_mask=attention_mask,\n",
    "                                max_new_tokens=max_gen_tokens)\n",
    "\n",
    "        generated_tokens = generated_tokens.to(\"cpu\")\n",
    "        predicted_names = tokenizer.batch_decode(generated_tokens[:, max_length:], skip_special_tokens=True)\n",
    "        predictions.extend(predicted_names)\n",
    "\n",
    "        # Free cuda memory\n",
    "        del generated_tokens\n",
    "        del inputs\n",
    "        del attention_mask\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def calculate_accuracy(true_tokens, predicted_tokens, tokenizer):\n",
    "    # Convert tokens to text\n",
    "    true_texts = [tokenizer.decode(tokens) for tokens in true_tokens]\n",
    "    predicted_texts = [tokenizer.decode(tokens) for tokens in predicted_tokens]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_texts, predicted_texts)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Tokenize\n",
    "def apply_template(x, tokenizer):\n",
    "    return {\"text\": tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"system\", \"content\": \"Generate a fitting name for the provided function. Place the suggested function name inside tripplet backtics. E.g ```adder```\"},{\"role\": \"user\", \"content\": x[\"source_code\"]}],\n",
    "        tokenize=False)\n",
    "    }\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_function_name(text):\n",
    "    match = re.search(r\"```(.*?)```\", text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return text\n",
    "\n",
    "\n",
    "def evaluate_with_model(model, tokenizer, dataset, batch_size: int, max_length: int = 2048, max_gen_tokens=100, transform_predictions=lambda x: x):\n",
    "    # model = None\n",
    "    predictions = predict_with_model(model, tokenizer, dataset, batch_size, max_length, max_gen_tokens)\n",
    "    predictions = transform_predictions(predictions)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(predictions, dataset[\"function_name\"]),\n",
    "        \"predictions\": predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.83s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Since we are able to bit the model in memory, we don't have to go for peft\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dst[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]\n",
      "No chat template is defined for this tokenizer - using the default template for the CodeLlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 8368.59 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 2000/2000 [00:00<00:00, 2133.56 examples/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 1/500 [00:03<25:25,  3.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 2/500 [00:05<22:43,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 3/500 [00:08<22:47,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 4/500 [00:11<22:31,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 5/500 [00:15<28:07,  3.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 6/500 [00:22<36:48,  4.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▏         | 7/500 [00:28<42:16,  5.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 8/500 [00:35<45:48,  5.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 9/500 [00:37<37:54,  4.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 10/500 [00:43<39:53,  4.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 11/500 [00:49<43:05,  5.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 12/500 [00:55<44:10,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 13/500 [00:57<37:24,  4.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 14/500 [01:00<32:45,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 15/500 [01:05<35:24,  4.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 16/500 [01:12<40:36,  5.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 17/500 [01:18<44:14,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|▎         | 18/500 [01:25<45:36,  5.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|▍         | 19/500 [01:27<38:01,  4.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|▍         | 20/500 [01:34<42:21,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|▍         | 21/500 [01:40<45:21,  5.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|▍         | 22/500 [01:46<44:42,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|▍         | 23/500 [01:52<46:58,  5.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|▍         | 24/500 [01:55<38:55,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|▌         | 25/500 [01:58<33:31,  4.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|▌         | 26/500 [02:02<34:22,  4.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|▌         | 27/500 [02:09<39:37,  5.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|▌         | 28/500 [02:15<43:16,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|▌         | 29/500 [02:22<45:47,  5.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|▌         | 30/500 [02:29<47:30,  6.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|▌         | 31/500 [02:34<46:09,  5.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|▋         | 32/500 [02:37<38:18,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  7%|▋         | 33/500 [02:43<42:13,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  7%|▋         | 34/500 [02:49<42:09,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  7%|▋         | 35/500 [02:55<44:49,  5.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  7%|▋         | 36/500 [03:01<43:18,  5.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  7%|▋         | 37/500 [03:03<36:10,  4.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|▊         | 38/500 [03:06<31:09,  4.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|▊         | 39/500 [03:12<37:00,  4.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|▊         | 40/500 [03:19<41:03,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|▊         | 41/500 [03:26<43:51,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|▊         | 42/500 [03:28<36:50,  4.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9%|▊         | 43/500 [03:31<31:44,  4.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9%|▉         | 44/500 [03:37<35:21,  4.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9%|▉         | 45/500 [03:43<38:19,  5.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9%|▉         | 46/500 [03:45<32:34,  4.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9%|▉         | 47/500 [03:50<34:05,  4.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|▉         | 48/500 [03:55<34:50,  4.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|▉         | 49/500 [03:58<30:09,  4.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|█         | 50/500 [04:04<35:59,  4.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|█         | 51/500 [04:07<31:08,  4.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|█         | 52/500 [04:10<27:44,  3.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11%|█         | 53/500 [04:12<25:20,  3.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11%|█         | 54/500 [04:15<23:44,  3.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11%|█         | 55/500 [04:20<28:16,  3.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11%|█         | 56/500 [04:27<34:25,  4.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11%|█▏        | 57/500 [04:33<37:20,  5.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|█▏        | 58/500 [04:38<37:52,  5.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|█▏        | 59/500 [04:45<41:01,  5.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|█▏        | 60/500 [04:51<43:11,  5.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|█▏        | 61/500 [04:54<35:43,  4.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|█▏        | 62/500 [04:57<30:52,  4.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 13%|█▎        | 63/500 [05:03<36:00,  4.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 13%|█▎        | 64/500 [05:08<35:31,  4.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 13%|█▎        | 65/500 [05:11<30:30,  4.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 13%|█▎        | 66/500 [05:17<35:38,  4.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 13%|█▎        | 67/500 [05:22<34:53,  4.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14%|█▎        | 68/500 [05:28<38:38,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14%|█▍        | 69/500 [05:33<37:42,  5.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14%|█▍        | 70/500 [05:40<40:14,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14%|█▍        | 71/500 [05:47<42:31,  5.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14%|█▍        | 72/500 [05:53<43:17,  6.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|█▍        | 73/500 [06:00<44:19,  6.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|█▍        | 74/500 [06:06<45:02,  6.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|█▌        | 75/500 [06:09<36:56,  5.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|█▌        | 76/500 [06:11<31:17,  4.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|█▌        | 77/500 [06:18<35:50,  5.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|█▌        | 78/500 [06:24<37:23,  5.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|█▌        | 79/500 [06:30<40:01,  5.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|█▌        | 80/500 [06:37<41:11,  5.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|█▌        | 81/500 [06:39<34:23,  4.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|█▋        | 82/500 [06:46<37:18,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 17%|█▋        | 83/500 [06:52<39:49,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 17%|█▋        | 84/500 [06:59<41:33,  5.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 17%|█▋        | 85/500 [07:03<37:30,  5.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 17%|█▋        | 86/500 [07:10<39:52,  5.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 17%|█▋        | 87/500 [07:16<40:36,  5.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18%|█▊        | 88/500 [07:19<33:41,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18%|█▊        | 89/500 [07:21<29:05,  4.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18%|█▊        | 90/500 [07:27<32:30,  4.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18%|█▊        | 91/500 [07:32<33:20,  4.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18%|█▊        | 92/500 [07:39<36:35,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19%|█▊        | 93/500 [07:46<38:59,  5.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19%|█▉        | 94/500 [07:52<40:38,  6.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19%|█▉        | 95/500 [07:55<33:35,  4.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19%|█▉        | 96/500 [08:01<36:49,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19%|█▉        | 97/500 [08:04<31:16,  4.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|█▉        | 98/500 [08:07<27:01,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|█▉        | 99/500 [08:09<24:02,  3.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|██        | 100/500 [08:14<26:13,  3.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|██        | 101/500 [08:19<28:18,  4.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|██        | 102/500 [08:26<33:10,  5.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 21%|██        | 103/500 [08:28<28:24,  4.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 21%|██        | 104/500 [08:35<33:11,  5.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 21%|██        | 105/500 [08:38<28:17,  4.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 21%|██        | 106/500 [08:44<31:18,  4.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 21%|██▏       | 107/500 [08:50<34:08,  5.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 22%|██▏       | 108/500 [08:56<36:48,  5.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 22%|██▏       | 109/500 [08:59<30:44,  4.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 22%|██▏       | 110/500 [09:06<34:21,  5.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 22%|██▏       | 111/500 [09:08<29:04,  4.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 22%|██▏       | 112/500 [09:14<32:11,  4.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 23%|██▎       | 113/500 [09:17<27:29,  4.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 23%|██▎       | 114/500 [09:20<24:35,  3.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 23%|██▎       | 115/500 [09:23<22:32,  3.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 23%|██▎       | 116/500 [09:29<28:25,  4.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 23%|██▎       | 117/500 [09:32<24:36,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24%|██▎       | 118/500 [09:34<22:02,  3.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24%|██▍       | 119/500 [09:37<20:08,  3.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24%|██▍       | 120/500 [09:43<26:36,  4.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24%|██▍       | 121/500 [09:46<23:32,  3.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24%|██▍       | 122/500 [09:52<28:05,  4.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25%|██▍       | 123/500 [09:55<24:42,  3.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25%|██▍       | 124/500 [10:00<27:01,  4.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25%|██▌       | 125/500 [10:07<31:15,  5.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25%|██▌       | 126/500 [10:09<26:38,  4.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25%|██▌       | 127/500 [10:12<23:19,  3.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26%|██▌       | 128/500 [10:18<28:36,  4.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26%|██▌       | 129/500 [10:24<31:20,  5.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26%|██▌       | 130/500 [10:31<34:06,  5.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26%|██▌       | 131/500 [10:37<34:08,  5.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26%|██▋       | 132/500 [10:39<28:30,  4.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 27%|██▋       | 133/500 [10:46<32:02,  5.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 27%|██▋       | 134/500 [10:52<33:39,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 27%|██▋       | 135/500 [10:59<35:34,  5.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 27%|██▋       | 136/500 [11:05<36:54,  6.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 27%|██▋       | 137/500 [11:10<34:52,  5.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28%|██▊       | 138/500 [11:17<36:18,  6.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28%|██▊       | 139/500 [11:22<34:58,  5.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28%|██▊       | 140/500 [11:25<29:14,  4.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28%|██▊       | 141/500 [11:27<25:05,  4.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28%|██▊       | 142/500 [11:30<22:09,  3.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 29%|██▊       | 143/500 [11:33<20:23,  3.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 29%|██▉       | 144/500 [11:39<26:00,  4.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 29%|██▉       | 145/500 [11:46<29:52,  5.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 29%|██▉       | 146/500 [11:53<32:32,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 29%|██▉       | 147/500 [11:59<34:23,  5.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|██▉       | 148/500 [12:02<28:28,  4.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|██▉       | 149/500 [12:08<31:28,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|███       | 150/500 [12:14<32:21,  5.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|███       | 151/500 [12:19<31:05,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|███       | 152/500 [12:25<32:30,  5.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31%|███       | 153/500 [12:31<31:52,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31%|███       | 154/500 [12:37<33:40,  5.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31%|███       | 155/500 [12:40<28:01,  4.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31%|███       | 156/500 [12:46<29:37,  5.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31%|███▏      | 157/500 [12:52<32:00,  5.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32%|███▏      | 158/500 [12:55<26:35,  4.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32%|███▏      | 159/500 [12:58<23:06,  4.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32%|███▏      | 160/500 [13:04<27:21,  4.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32%|███▏      | 161/500 [13:09<27:35,  4.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32%|███▏      | 162/500 [13:14<28:03,  4.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 33%|███▎      | 163/500 [13:17<24:03,  4.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 33%|███▎      | 164/500 [13:20<21:20,  3.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 33%|███▎      | 165/500 [13:25<23:36,  4.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 33%|███▎      | 166/500 [13:31<27:21,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 33%|███▎      | 167/500 [13:34<23:23,  4.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|███▎      | 168/500 [13:39<25:18,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|███▍      | 169/500 [13:42<21:55,  3.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|███▍      | 170/500 [13:49<26:12,  4.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|███▍      | 171/500 [13:51<22:31,  4.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|███▍      | 172/500 [13:57<25:02,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|███▍      | 173/500 [14:03<28:16,  5.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|███▍      | 174/500 [14:10<30:29,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|███▌      | 175/500 [14:15<29:43,  5.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|███▌      | 176/500 [14:22<31:26,  5.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|███▌      | 177/500 [14:29<32:35,  6.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36%|███▌      | 178/500 [14:35<33:22,  6.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36%|███▌      | 179/500 [14:42<33:54,  6.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36%|███▌      | 180/500 [14:48<34:15,  6.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36%|███▌      | 181/500 [14:55<34:26,  6.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36%|███▋      | 182/500 [15:02<34:33,  6.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 37%|███▋      | 183/500 [15:08<34:14,  6.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 37%|███▋      | 184/500 [15:15<34:21,  6.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 37%|███▋      | 185/500 [15:21<34:23,  6.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 37%|███▋      | 186/500 [15:28<34:23,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 37%|███▋      | 187/500 [15:34<34:20,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|███▊      | 188/500 [15:40<32:05,  6.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|███▊      | 189/500 [15:46<32:40,  6.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|███▊      | 190/500 [15:49<26:52,  5.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|███▊      | 191/500 [15:55<28:53,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|███▊      | 192/500 [16:02<30:20,  5.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39%|███▊      | 193/500 [16:05<25:16,  4.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39%|███▉      | 194/500 [16:10<25:36,  5.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39%|███▉      | 195/500 [16:15<25:10,  4.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39%|███▉      | 196/500 [16:21<27:36,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39%|███▉      | 197/500 [16:27<27:44,  5.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|███▉      | 198/500 [16:30<23:18,  4.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|███▉      | 199/500 [16:32<20:16,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|████      | 200/500 [16:39<24:04,  4.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|████      | 201/500 [16:42<20:53,  4.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|████      | 202/500 [16:48<24:24,  4.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41%|████      | 203/500 [16:55<26:51,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41%|████      | 204/500 [16:57<22:32,  4.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41%|████      | 205/500 [17:04<25:28,  5.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41%|████      | 206/500 [17:11<27:34,  5.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41%|████▏     | 207/500 [17:13<22:57,  4.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|████▏     | 208/500 [17:20<25:38,  5.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|████▏     | 209/500 [17:22<21:46,  4.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|████▏     | 210/500 [17:29<24:03,  4.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|████▏     | 211/500 [17:34<24:13,  5.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|████▏     | 212/500 [17:40<26:24,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 43%|████▎     | 213/500 [17:46<27:00,  5.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 43%|████▎     | 214/500 [17:53<28:18,  5.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 43%|████▎     | 215/500 [17:55<23:14,  4.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 43%|████▎     | 216/500 [18:02<25:35,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 43%|████▎     | 217/500 [18:09<27:12,  5.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|████▎     | 218/500 [18:11<22:38,  4.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|████▍     | 219/500 [18:18<25:05,  5.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|████▍     | 220/500 [18:24<26:45,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|████▍     | 221/500 [18:31<27:53,  6.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|████▍     | 222/500 [18:38<28:39,  6.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|████▍     | 223/500 [18:40<23:37,  5.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|████▍     | 224/500 [18:47<25:37,  5.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|████▌     | 225/500 [18:54<26:58,  5.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|████▌     | 226/500 [19:00<27:02,  5.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|████▌     | 227/500 [19:02<22:26,  4.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 46%|████▌     | 228/500 [19:09<24:38,  5.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 46%|████▌     | 229/500 [19:15<26:09,  5.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 46%|████▌     | 230/500 [19:18<21:37,  4.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 46%|████▌     | 231/500 [19:21<18:37,  4.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 46%|████▋     | 232/500 [19:23<16:34,  3.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 47%|████▋     | 233/500 [19:30<20:24,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 47%|████▋     | 234/500 [19:36<23:02,  5.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 47%|████▋     | 235/500 [19:42<23:58,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 47%|████▋     | 236/500 [19:49<25:27,  5.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 47%|████▋     | 237/500 [19:52<21:19,  4.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48%|████▊     | 238/500 [19:58<23:31,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48%|████▊     | 239/500 [20:05<25:02,  5.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48%|████▊     | 240/500 [20:12<26:05,  6.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48%|████▊     | 241/500 [20:18<26:45,  6.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48%|████▊     | 242/500 [20:25<27:11,  6.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 49%|████▊     | 243/500 [20:28<22:23,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 49%|████▉     | 244/500 [20:34<24:04,  5.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 49%|████▉     | 245/500 [20:37<20:12,  4.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 49%|████▉     | 246/500 [20:42<20:59,  4.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 49%|████▉     | 247/500 [20:49<23:00,  5.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|████▉     | 248/500 [20:56<24:23,  5.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|████▉     | 249/500 [21:02<25:18,  6.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|█████     | 250/500 [21:09<25:55,  6.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|█████     | 251/500 [21:12<21:33,  5.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|█████     | 252/500 [21:18<23:13,  5.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 51%|█████     | 253/500 [21:25<24:23,  5.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 51%|█████     | 254/500 [21:31<25:08,  6.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 51%|█████     | 255/500 [21:34<20:58,  5.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 51%|█████     | 256/500 [21:41<22:42,  5.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 51%|█████▏    | 257/500 [21:47<23:51,  5.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52%|█████▏    | 258/500 [21:54<24:39,  6.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52%|█████▏    | 259/500 [22:01<25:10,  6.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52%|█████▏    | 260/500 [22:07<25:28,  6.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52%|█████▏    | 261/500 [22:14<25:40,  6.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52%|█████▏    | 262/500 [22:21<25:47,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 53%|█████▎    | 263/500 [22:27<25:48,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 53%|█████▎    | 264/500 [22:30<21:08,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 53%|█████▎    | 265/500 [22:33<18:04,  4.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 53%|█████▎    | 266/500 [22:35<15:50,  4.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 53%|█████▎    | 267/500 [22:42<18:44,  4.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 54%|█████▎    | 268/500 [22:49<20:44,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 54%|█████▍    | 269/500 [22:55<22:06,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 54%|█████▍    | 270/500 [23:01<21:30,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 54%|█████▍    | 271/500 [23:03<18:02,  4.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 54%|█████▍    | 272/500 [23:09<18:42,  4.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|█████▍    | 273/500 [23:15<20:32,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|█████▍    | 274/500 [23:21<20:18,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|█████▌    | 275/500 [23:27<21:29,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|█████▌    | 276/500 [23:34<22:22,  5.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|█████▌    | 277/500 [23:40<22:58,  6.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|█████▌    | 278/500 [23:47<23:20,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|█████▌    | 279/500 [23:50<19:13,  5.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|█████▌    | 280/500 [23:56<20:40,  5.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|█████▌    | 281/500 [24:03<21:38,  5.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|█████▋    | 282/500 [24:06<18:12,  5.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 57%|█████▋    | 283/500 [24:08<15:34,  4.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 57%|█████▋    | 284/500 [24:11<13:38,  3.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 57%|█████▋    | 285/500 [24:17<16:02,  4.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 57%|█████▋    | 286/500 [24:20<14:09,  3.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 57%|█████▋    | 287/500 [24:23<12:48,  3.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 58%|█████▊    | 288/500 [24:29<15:56,  4.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 58%|█████▊    | 289/500 [24:32<14:22,  4.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 58%|█████▊    | 290/500 [24:35<12:37,  3.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 58%|█████▊    | 291/500 [24:38<11:41,  3.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 58%|█████▊    | 292/500 [24:40<10:57,  3.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59%|█████▊    | 293/500 [24:47<14:28,  4.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59%|█████▉    | 294/500 [24:53<16:53,  4.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59%|█████▉    | 295/500 [25:00<18:32,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59%|█████▉    | 296/500 [25:07<19:39,  5.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59%|█████▉    | 297/500 [25:09<16:18,  4.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|█████▉    | 298/500 [25:14<16:22,  4.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|█████▉    | 299/500 [25:21<18:02,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|██████    | 300/500 [25:27<19:10,  5.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|██████    | 301/500 [25:30<16:18,  4.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|██████    | 302/500 [25:37<17:53,  5.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61%|██████    | 303/500 [25:40<15:08,  4.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61%|██████    | 304/500 [25:46<17:01,  5.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61%|██████    | 305/500 [25:53<18:17,  5.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61%|██████    | 306/500 [26:00<19:09,  5.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61%|██████▏   | 307/500 [26:06<19:42,  6.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|██████▏   | 308/500 [26:13<20:04,  6.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|██████▏   | 309/500 [26:19<20:18,  6.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|██████▏   | 310/500 [26:26<20:25,  6.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|██████▏   | 311/500 [26:33<20:28,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|██████▏   | 312/500 [26:39<20:28,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 63%|██████▎   | 313/500 [26:42<16:46,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 63%|██████▎   | 314/500 [26:44<14:02,  4.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 63%|██████▎   | 315/500 [26:47<12:19,  3.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 63%|██████▎   | 316/500 [26:50<10:59,  3.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 63%|██████▎   | 317/500 [26:55<12:46,  4.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 64%|██████▎   | 318/500 [27:02<14:54,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 64%|██████▍   | 319/500 [27:09<16:22,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 64%|██████▍   | 320/500 [27:15<17:20,  5.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 64%|██████▍   | 321/500 [27:21<16:48,  5.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 64%|██████▍   | 322/500 [27:27<17:35,  5.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|██████▍   | 323/500 [27:32<16:38,  5.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|██████▍   | 324/500 [27:39<17:25,  5.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|██████▌   | 325/500 [27:44<16:59,  5.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|██████▌   | 326/500 [27:51<17:34,  6.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|██████▌   | 327/500 [27:54<14:30,  5.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|██████▌   | 328/500 [28:00<15:46,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|██████▌   | 329/500 [28:03<13:13,  4.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|██████▌   | 330/500 [28:09<14:49,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|██████▌   | 331/500 [28:16<15:54,  5.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|██████▋   | 332/500 [28:23<16:37,  5.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 67%|██████▋   | 333/500 [28:29<17:04,  6.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 67%|██████▋   | 334/500 [28:36<17:22,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 67%|██████▋   | 335/500 [28:39<14:19,  5.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 67%|██████▋   | 336/500 [28:45<15:23,  5.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 67%|██████▋   | 337/500 [28:48<12:50,  4.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68%|██████▊   | 338/500 [28:54<13:53,  5.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68%|██████▊   | 339/500 [29:01<14:59,  5.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68%|██████▊   | 340/500 [29:03<12:27,  4.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68%|██████▊   | 341/500 [29:10<13:55,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68%|██████▊   | 342/500 [29:16<14:54,  5.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69%|██████▊   | 343/500 [29:23<15:33,  5.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69%|██████▉   | 344/500 [29:30<15:58,  6.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69%|██████▉   | 345/500 [29:36<16:14,  6.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69%|██████▉   | 346/500 [29:43<16:22,  6.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69%|██████▉   | 347/500 [29:49<16:26,  6.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|██████▉   | 348/500 [29:56<16:27,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|██████▉   | 349/500 [29:59<13:21,  5.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|███████   | 350/500 [30:01<11:09,  4.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|███████   | 351/500 [30:08<12:40,  5.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|███████   | 352/500 [30:14<13:42,  5.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 71%|███████   | 353/500 [30:21<14:24,  5.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 71%|███████   | 354/500 [30:27<14:49,  6.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 71%|███████   | 355/500 [30:30<12:18,  5.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 71%|███████   | 356/500 [30:36<13:01,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 71%|███████▏  | 357/500 [30:39<10:59,  4.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72%|███████▏  | 358/500 [30:44<11:25,  4.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72%|███████▏  | 359/500 [30:51<12:36,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72%|███████▏  | 360/500 [30:58<13:23,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72%|███████▏  | 361/500 [31:04<13:54,  6.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72%|███████▏  | 362/500 [31:07<11:29,  5.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 73%|███████▎  | 363/500 [31:14<12:31,  5.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 73%|███████▎  | 364/500 [31:19<12:39,  5.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 73%|███████▎  | 365/500 [31:26<13:15,  5.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 73%|███████▎  | 366/500 [31:31<12:18,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 73%|███████▎  | 367/500 [31:33<10:24,  4.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74%|███████▎  | 368/500 [31:36<09:07,  4.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74%|███████▍  | 369/500 [31:39<08:03,  3.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74%|███████▍  | 370/500 [31:42<07:23,  3.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74%|███████▍  | 371/500 [31:44<06:47,  3.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74%|███████▍  | 372/500 [31:47<06:23,  3.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75%|███████▍  | 373/500 [31:49<06:04,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75%|███████▍  | 374/500 [31:52<05:54,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75%|███████▌  | 375/500 [31:59<08:13,  3.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75%|███████▌  | 376/500 [32:05<09:48,  4.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75%|███████▌  | 377/500 [32:08<08:31,  4.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76%|███████▌  | 378/500 [32:15<09:57,  4.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76%|███████▌  | 379/500 [32:21<10:54,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76%|███████▌  | 380/500 [32:28<11:32,  5.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76%|███████▌  | 381/500 [32:35<11:56,  6.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76%|███████▋  | 382/500 [32:38<10:02,  5.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 77%|███████▋  | 383/500 [32:40<08:36,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 77%|███████▋  | 384/500 [32:47<09:48,  5.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 77%|███████▋  | 385/500 [32:54<10:36,  5.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 77%|███████▋  | 386/500 [32:56<08:54,  4.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 77%|███████▋  | 387/500 [32:59<07:48,  4.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78%|███████▊  | 388/500 [33:06<09:07,  4.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78%|███████▊  | 389/500 [33:12<10:00,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78%|███████▊  | 390/500 [33:15<08:21,  4.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78%|███████▊  | 391/500 [33:22<09:24,  5.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78%|███████▊  | 392/500 [33:28<10:05,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 79%|███████▊  | 393/500 [33:35<10:31,  5.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 79%|███████▉  | 394/500 [33:38<08:44,  4.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 79%|███████▉  | 395/500 [33:40<07:25,  4.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 79%|███████▉  | 396/500 [33:47<08:35,  4.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 79%|███████▉  | 397/500 [33:49<07:19,  4.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|███████▉  | 398/500 [33:56<08:26,  4.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|███████▉  | 399/500 [33:59<07:20,  4.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|████████  | 400/500 [34:02<06:27,  3.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|████████  | 401/500 [34:08<07:45,  4.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|████████  | 402/500 [34:15<08:36,  5.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81%|████████  | 403/500 [34:22<09:10,  5.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81%|████████  | 404/500 [34:28<09:32,  5.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81%|████████  | 405/500 [34:35<09:44,  6.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81%|████████  | 406/500 [34:41<09:32,  6.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81%|████████▏ | 407/500 [34:43<07:47,  5.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82%|████████▏ | 408/500 [34:46<06:33,  4.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82%|████████▏ | 409/500 [34:48<05:42,  3.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82%|████████▏ | 410/500 [34:55<06:55,  4.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82%|████████▏ | 411/500 [35:01<07:32,  5.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82%|████████▏ | 412/500 [35:08<08:07,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 83%|████████▎ | 413/500 [35:13<07:50,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 83%|████████▎ | 414/500 [35:19<08:16,  5.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 83%|████████▎ | 415/500 [35:26<08:31,  6.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 83%|████████▎ | 416/500 [35:33<08:40,  6.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 83%|████████▎ | 417/500 [35:35<07:02,  5.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|████████▎ | 418/500 [35:38<05:58,  4.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|████████▍ | 419/500 [35:44<06:48,  5.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|████████▍ | 420/500 [35:51<07:13,  5.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|████████▍ | 421/500 [35:53<06:00,  4.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|████████▍ | 422/500 [35:56<05:08,  3.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|████████▍ | 423/500 [36:03<06:06,  4.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|████████▍ | 424/500 [36:08<06:14,  4.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|████████▌ | 425/500 [36:14<06:37,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|████████▌ | 426/500 [36:17<05:29,  4.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|████████▌ | 427/500 [36:23<06:12,  5.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86%|████████▌ | 428/500 [36:30<06:39,  5.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86%|████████▌ | 429/500 [36:32<05:31,  4.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86%|████████▌ | 430/500 [36:39<06:08,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86%|████████▌ | 431/500 [36:46<06:30,  5.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86%|████████▋ | 432/500 [36:50<06:01,  5.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 87%|████████▋ | 433/500 [36:57<06:21,  5.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 87%|████████▋ | 434/500 [37:03<06:34,  5.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 87%|████████▋ | 435/500 [37:09<06:28,  5.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 87%|████████▋ | 436/500 [37:16<06:34,  6.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 87%|████████▋ | 437/500 [37:21<06:00,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|████████▊ | 438/500 [37:23<04:57,  4.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|████████▊ | 439/500 [37:30<05:25,  5.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|████████▊ | 440/500 [37:36<05:43,  5.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|████████▊ | 441/500 [37:39<04:42,  4.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|████████▊ | 442/500 [37:46<05:09,  5.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89%|████████▊ | 443/500 [37:52<05:25,  5.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89%|████████▉ | 444/500 [37:56<04:45,  5.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89%|████████▉ | 445/500 [37:58<03:58,  4.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89%|████████▉ | 446/500 [38:05<04:31,  5.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89%|████████▉ | 447/500 [38:11<04:42,  5.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|████████▉ | 448/500 [38:14<03:55,  4.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|████████▉ | 449/500 [38:17<03:23,  3.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|█████████ | 450/500 [38:19<02:59,  3.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|█████████ | 451/500 [38:22<02:41,  3.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|█████████ | 452/500 [38:24<02:28,  3.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91%|█████████ | 453/500 [38:27<02:20,  3.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91%|█████████ | 454/500 [38:30<02:13,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91%|█████████ | 455/500 [38:32<02:06,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91%|█████████ | 456/500 [38:39<02:54,  3.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91%|█████████▏| 457/500 [38:46<03:24,  4.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|█████████▏| 458/500 [38:51<03:29,  4.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|█████████▏| 459/500 [38:57<03:38,  5.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|█████████▏| 460/500 [39:04<03:48,  5.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|█████████▏| 461/500 [39:11<03:53,  5.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|█████████▏| 462/500 [39:17<03:54,  6.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 93%|█████████▎| 463/500 [39:20<03:09,  5.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 93%|█████████▎| 464/500 [39:23<02:37,  4.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 93%|█████████▎| 465/500 [39:25<02:14,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 93%|█████████▎| 466/500 [39:28<01:58,  3.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 93%|█████████▎| 467/500 [39:30<01:46,  3.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|█████████▎| 468/500 [39:37<02:15,  4.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|█████████▍| 469/500 [39:44<02:33,  4.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|█████████▍| 470/500 [39:50<02:43,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|█████████▍| 471/500 [39:56<02:42,  5.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|█████████▍| 472/500 [40:03<02:45,  5.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|█████████▍| 473/500 [40:05<02:12,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|█████████▍| 474/500 [40:12<02:20,  5.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|█████████▌| 475/500 [40:19<02:24,  5.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|█████████▌| 476/500 [40:21<01:55,  4.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|█████████▌| 477/500 [40:24<01:35,  4.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 96%|█████████▌| 478/500 [40:30<01:47,  4.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 96%|█████████▌| 479/500 [40:33<01:29,  4.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 96%|█████████▌| 480/500 [40:38<01:28,  4.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 96%|█████████▌| 481/500 [40:45<01:36,  5.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 96%|█████████▋| 482/500 [40:51<01:39,  5.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 97%|█████████▋| 483/500 [40:54<01:19,  4.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 97%|█████████▋| 484/500 [41:00<01:23,  5.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 97%|█████████▋| 485/500 [41:03<01:07,  4.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 97%|█████████▋| 486/500 [41:10<01:11,  5.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 97%|█████████▋| 487/500 [41:12<00:56,  4.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98%|█████████▊| 488/500 [41:15<00:45,  3.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98%|█████████▊| 489/500 [41:17<00:37,  3.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98%|█████████▊| 490/500 [41:24<00:43,  4.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98%|█████████▊| 491/500 [41:27<00:34,  3.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98%|█████████▊| 492/500 [41:33<00:37,  4.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 99%|█████████▊| 493/500 [41:36<00:28,  4.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 99%|█████████▉| 494/500 [41:43<00:29,  4.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 99%|█████████▉| 495/500 [41:49<00:26,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 99%|█████████▉| 496/500 [41:56<00:22,  5.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 99%|█████████▉| 497/500 [42:01<00:16,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|█████████▉| 498/500 [42:04<00:09,  4.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|█████████▉| 499/500 [42:10<00:05,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|██████████| 500/500 [42:17<00:00,  5.07s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = dst[\"test\"].map(apply_template, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "results = evaluate_with_model(model, tokenizer, dataset, 4, transform_predictions=lambda x: [extract_function_name(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['youtube_id_extractor',\n",
       " 'url_extractor',\n",
       " 'hasher',\n",
       " 'fc2video_downloader',\n",
       " 'download_video',\n",
       " 'xml_to_dict',\n",
       " 'ucas_downloader',\n",
       " 'ucas_downloader',\n",
       " 'download_video',\n",
       " 'sina_flv_downloader',\n",
       " 'sina_video_downloader',\n",
       " 'yixia_miaopai_downloader',\n",
       " 'veoh_downloader',\n",
       " 'veoh_downloader',\n",
       " 'downloader',\n",
       " 'get_room_id_from_url',\n",
       " 'colorizer',\n",
       " 'colorizer',\n",
       " 'warning',\n",
       " 'error',\n",
       " ' The provided function is a way to determine the operating system (OS) of the system it is running on. The function takes no arguments and returns a string representing the OS.\\n\\nA fitting name for this function could be `get_os()`. This name is descriptive and concise, and it clearly conveys the purpose of the function.\\n\\nHere is the function with the suggested name:\\n```\\ndef get_os():\\n    syst = system().lower()\\n',\n",
       " 'weibo_downloader',\n",
       " 'vimeo_downloader',\n",
       " 'vimeo_downloader',\n",
       " 'vimeo_downloader',\n",
       " 'ckplayer_info',\n",
       " 'decryptor',\n",
       " 'mgtv_video_id',\n",
       " 'm3u_downloader',\n",
       " 'git_branch_commit_parser',\n",
       " 'sanitize_filename',\n",
       " 'terminal_size',\n",
       " 'theplatform_downloader',\n",
       " 'download_video',\n",
       " 'acfun_downloader',\n",
       " 'get_options',\n",
       " 'downloader',\n",
       " 'extract_first_match',\n",
       " 'extract_matches',\n",
       " 'query_param_extractor',\n",
       " 'ungzip',\n",
       " 'decompressor',\n",
       " 'get_content',\n",
       " 'fetcher',\n",
       " 'hostname_and_port',\n",
       " 'fprint',\n",
       " 'get_room_id',\n",
       " 'showroom_live',\n",
       " 'topic_part_adder',\n",
       " 'download_by_course_topic_part',\n",
       " 'download_bokecc_video',\n",
       " 'get_video_sources',\n",
       " 'is_task_queued',\n",
       " 'clear_events',\n",
       " 'snowflake_conn_config',\n",
       " 'snowflake_uri_builder',\n",
       " 'snowflake_connector',\n",
       " 'snowflake_credentials',\n",
       " 'get_extra_grpc_field',\n",
       " ' The provided function is a method that performs a database operation by executing a SQL query and writing the results to a file. The function name could be `execute_sql_to_file`.',\n",
       " 'loader',\n",
       " 'copy_table_to_tmp_file',\n",
       " 'upload_to_gcs',\n",
       " 'max_partition_finder',\n",
       " \" The provided function takes three arguments: `target_dt`, `date_list`, and `before_target`. The function returns the date that is closest to `target_dt` in the `date_list`, with the option to prioritize dates that are before or after `target_dt`.\\n\\nBased on the function's behavior, I would suggest the following name:\\n```\\nclosest_date\\n```\\nThis name clearly conveys the function'\",\n",
       " 'find_closest_partition',\n",
       " 'mysql_connection',\n",
       " 'loader',\n",
       " 'object_counter',\n",
       " 'stack_traces',\n",
       " 'trigger_dag',\n",
       " 'drop_dag',\n",
       " 'check_dependencies',\n",
       " 'task_instance_state',\n",
       " 'dag_state',\n",
       " 'next_execution_time',\n",
       " \" The provided function appears to be a part of an Airflow webserver process management script. The function is responsible for monitoring the number of workers running in the webserver and refreshing them as needed.\\n\\nBased on the function's name and the context in which it is used, I would suggest the following name:\\n```\\ndef refresh_workers(gunicorn_master_proc, num_workers_expected, master_timeout):\\n```\\nThis\",\n",
       " 'client_getter',\n",
       " 'translator',\n",
       " \" The provided function appears to be a custom implementation of a Bash command runner. The function takes a Bash command as input and runs it in a subprocess, capturing the output and logging it.\\n\\nGiven this, a fitting name for the function could be `BashCommandRunner`. This name clearly conveys the purpose of the function, which is to run a Bash command and capture its output.\\n\\nHere's the suggested function name in triple backticks: ```\",\n",
       " 'get_instance',\n",
       " 'creator',\n",
       " 'updater',\n",
       " 'deleter',\n",
       " 'database_getter',\n",
       " 'create_database',\n",
       " 'database_patcher',\n",
       " 'deleter',\n",
       " 'export_instance',\n",
       " 'wait_for_cloud_sql_operation',\n",
       " 'start_proxy',\n",
       " 'cleanup',\n",
       " 'sql_proxy_version',\n",
       " 'connection_adder',\n",
       " 'get_connection',\n",
       " 'delete_connection',\n",
       " 'cloud_sql_proxy_runner',\n",
       " 'database_connection_hook',\n",
       " 'notice_handler',\n",
       " 'socket_creator',\n",
       " 'sanitize_job_id',\n",
       " 'error_code_extractor',\n",
       " 'register_sensors',\n",
       " 'deleter',\n",
       " 'task_instance_deleter',\n",
       " 'set_dags_paused',\n",
       " 'ti_performance',\n",
       " ' The provided function is a part of the Airflow scheduler, specifically the `SchedulerMetricsJob` class. The function is responsible for printing the scheduler metrics, which include the number of task instances that have been processed and the number of successful task instances.\\n\\nGiven this information, a fitting name for the function could be `print_scheduler_metrics`. This name clearly conveys the purpose of the function and is easy to understand.\\n\\nHere is',\n",
       " 'invoke_function',\n",
       " 'get_dagrun_state',\n",
       " ' The provided function is a custom operator for evaluating the performance of a machine learning model using the Google Cloud AI Platform. The function takes in several parameters, including the task prefix, data format, input paths, prediction path, metric function and keys, validate function, and other optional parameters such as project ID, region, and dataflow options.\\n\\nThe function first checks if the task prefix is a valid identifier and then creates three Airflow operators: an MLEngineBatchPred',\n",
       " 'mkdir',\n",
       " 'parses',\n",
       " 'timestamp',\n",
       " 'timestamp',\n",
       " 'localize_datetime',\n",
       " ' The provided function is a utility function for converting a naive datetime object to a timezone-aware datetime object.\\n\\nThe function takes two arguments: `value` and `timezone`. If `timezone` is not provided, it defaults to the `TIMEZONE` variable.\\n\\nThe function first checks if `value` is a naive datetime object. If it is, it raises a `ValueError` with a message indicating that `make_aware` expects a',\n",
       " 'localize',\n",
       " 'datetime_with_timezone',\n",
       " 'gcloud_credentials_manager',\n",
       " 'get_extra_field',\n",
       " 'druid_broker_connection',\n",
       " 'session_builder',\n",
       " 'fetcher',\n",
       " 'http_error_handler',\n",
       " 'retry_request',\n",
       " 'retryer',\n",
       " 'session_manager',\n",
       " 'session_wrapper',\n",
       " 'db_reset',\n",
       " 'upload_file',\n",
       " ' The provided function is a wrapper around the `presto.connect` function, which is used to establish a connection to a Presto database. The function takes a `self` parameter, which is likely an instance of a PrestoHook class.\\n\\nGiven this information, a fitting name for the function could be `presto_connect`. This name clearly conveys the purpose of the function, which is to establish a connection to a Presto database.\\n\\nHere',\n",
       " 'errorMessage',\n",
       " 'presto_query',\n",
       " 'query_executor',\n",
       " 'sql_runner',\n",
       " 'insert_rows',\n",
       " 'cosmos_client_factory',\n",
       " ' The provided function is a Python function that checks if a collection exists in a Cosmos DB database. The function takes two arguments: `collection_name` and `database_name`.\\n\\nThe suggested function name for this function is `collection_exists`. This name is descriptive and clearly conveys the purpose of the function. It also follows the Python naming conventions for functions.\\n\\nHere is the function with the suggested name:\\n```\\ndef collection_exists(self',\n",
       " ' The provided function is a method that creates a container in a Cosmos DB database if it does not already exist. The function takes two parameters: `collection_name` and `database_name`.\\n\\nGiven this information, a fitting name for the function could be:\\n```\\ncreate_container_if_not_exists\\n```\\nThis name clearly conveys the purpose of the function, which is to create a container in a Cosmos DB database if it does not',\n",
       " 'database_exists',\n",
       " 'create_database',\n",
       " 'delete_database',\n",
       " 'delete_collection',\n",
       " 'document_adder',\n",
       " 'document_adder',\n",
       " 'delete_document',\n",
       " 'get_document',\n",
       " 'query_items',\n",
       " 'dag_code',\n",
       " 'get_function',\n",
       " 'function_creator',\n",
       " 'function_updater',\n",
       " 'uploader',\n",
       " 'deleter',\n",
       " ' The provided function is a polling function that retrieves the result of an operation by repeatedly calling the `operations().get()` method of the Google Cloud API. The function takes an `operation_name` parameter and returns the result of the operation.\\n\\nA fitting name for this function could be `get_operation_result`. This name clearly conveys the purpose of the function and is easy to understand.\\n\\nHere is the suggested function name in triple backticks:',\n",
       " 'publisher',\n",
       " 'creator',\n",
       " 'deleter',\n",
       " 'creator',\n",
       " 'delete_subscription',\n",
       " 'receiver',\n",
       " 'acknowledge_messages',\n",
       " 'task_dependency_checker',\n",
       " 'is_all_dependencies_passed',\n",
       " 'check_dependencies',\n",
       " 's3_credentials',\n",
       " 'get_credentials',\n",
       " 'get_role_arn',\n",
       " 'vertica_connect',\n",
       " 'logger_context_setter',\n",
       " 'buffer_writer',\n",
       " 'buffer_flusher',\n",
       " 'get_archive_or_fileloc',\n",
       " 'dag_finder',\n",
       " 'get_task_instance',\n",
       " 'get_simple_dag',\n",
       " \" The function you provided is responsible for launching a DagFileProcessorManager process. Based on the function's implementation, a fitting name for the function could be:\\n```\\nlaunch_dag_file_processor_manager\\n```\\nThis name clearly conveys the purpose of the function, which is to launch a DagFileProcessorManager process.\",\n",
       " 'fetch_results',\n",
       " 'start_process',\n",
       " 'processor',\n",
       " 'terminate_manager',\n",
       " 'terminator',\n",
       " 'terminator',\n",
       " 'processor',\n",
       " 'dag_parser',\n",
       " 'dag_parser',\n",
       " 'dag_file_scanner',\n",
       " 'print_stats',\n",
       " 'cleaner',\n",
       " 'dag_processing_stats',\n",
       " 'file_processor',\n",
       " 'processor_age',\n",
       " 'get_start_time',\n",
       " 'file_path_manager',\n",
       " 'processor_runner',\n",
       " 'dag_processor',\n",
       " 'find_zombie_jobs',\n",
       " 'is_max_runs_reached',\n",
       " 'terminator',\n",
       " 'ssh_client_factory',\n",
       " 'ssh_tunnel',\n",
       " 'transfer_job_creator',\n",
       " 'get_transfer_job',\n",
       " 'transfer_jobs_list',\n",
       " 'transfer_job_patcher',\n",
       " 'delete_transfer_job',\n",
       " 'cancel_operation',\n",
       " 'transfer_operation',\n",
       " 'get_transfer_operations',\n",
       " 'pauser',\n",
       " 'resumer',\n",
       " 'wait_for_transfer_operation',\n",
       " 'has_unexpected_status',\n",
       " 'get_task_reschedules',\n",
       " 'camelCaseToSnakeCase',\n",
       " 'safe_pod_id_gen',\n",
       " 'sanitize_label',\n",
       " 'task_instance_adder',\n",
       " 'slot_availability',\n",
       " 'interpolator',\n",
       " 'process_command',\n",
       " 'interpolator',\n",
       " 'remover',\n",
       " 'get_airflow_config_section',\n",
       " 'config_options',\n",
       " 'allocator',\n",
       " 'beginTransaction',\n",
       " 'commit_project',\n",
       " 'get_keys',\n",
       " 'rollback_transaction',\n",
       " 'query_runner',\n",
       " 'get_operation',\n",
       " 'delete_operation',\n",
       " 'poller',\n",
       " 'export_entities',\n",
       " 'importer',\n",
       " 'publish_message',\n",
       " ' The provided function is a custom implementation of a hostname resolver. It first tries to get the hostname from the Airflow configuration file, and if that fails, it falls back to using the `socket.getfqdn()` function to get the fully qualified domain name of the current machine.\\n\\nThe function name `hostname_resolver` is a good fit for this function, as it clearly conveys the purpose of the function and is easy to understand.\\n\\n',\n",
       " 'get_connection',\n",
       " 'entity_analyzer',\n",
       " 'text_annotator',\n",
       " 'classifier',\n",
       " 'get_task',\n",
       " 'get_template_fields',\n",
       " 'get_template_field',\n",
       " 'dispose_db_connection_pool',\n",
       " 'airflow_path_adder',\n",
       " 'is_ready',\n",
       " 'is_kerberos_ccache_present',\n",
       " 'serializer',\n",
       " 'chunk_iterator',\n",
       " 'accumulator',\n",
       " 'dag_builder',\n",
       " 'pretty_table',\n",
       " 'terminate_process',\n",
       " 'filename_template_renderer',\n",
       " 'get_task_instance',\n",
       " 'operator_loader',\n",
       " 'dataproc_client',\n",
       " 'submit_operation',\n",
       " 'json_serializer',\n",
       " \" The provided function is a part of the Apache Airflow framework, and it is responsible for monitoring the state of a Spark job and waiting for it to complete.\\n\\nGiven this, a fitting name for the function could be `spark_job_monitor`. This name suggests that the function is monitoring the state of a Spark job, and it is a concise and descriptive name that accurately reflects the function's purpose.\\n\\nHere is the suggested function name in\",\n",
       " 'pig_runner',\n",
       " 'celery_task_status',\n",
       " 'batch_size',\n",
       " 'task_batch_size',\n",
       " 'get_or_set',\n",
       " 'ml_client',\n",
       " 'creator',\n",
       " 'get_mlengine_job',\n",
       " 'wait_for_job',\n",
       " 'create_model_version',\n",
       " 'set_default_version',\n",
       " 'list_model_versions',\n",
       " 'deleter',\n",
       " 'model_creator',\n",
       " 'get_model',\n",
       " 'execute_command',\n",
       " 'dynamodb_batch_writer',\n",
       " 'executor_loader',\n",
       " 'get_default_executor',\n",
       " ' The provided function is responsible for creating an instance of an executor based on the name of the executor. The function takes an `executor_name` parameter and returns an instance of the corresponding executor class.\\n\\nThe function first checks if the executor name is one of the built-in executors (LocalExecutor, SequentialExecutor, CeleryExecutor, DaskExecutor, or KubernetesExecutor). If it is, it returns an instance of the corresponding executor',\n",
       " 'segment_error_handler',\n",
       " 'start_pod',\n",
       " 'mssql_connection',\n",
       " 'submit_application',\n",
       " 'trigger_dag',\n",
       " 'deleter',\n",
       " 'get_dag_runs_by_state',\n",
       " 'get_code',\n",
       " 'get_task_info',\n",
       " 'pause_dag',\n",
       " \" The provided function is a web service that retrieves information about a specific task instance in Airflow. The function takes three parameters: `dag_id`, `execution_date`, and `task_id`.\\n\\nBased on the function's purpose and the parameters it takes, a fitting name for the function could be:\\n```\\nget_task_instance_info\\n```\\nThis name clearly conveys the purpose of the function and the information it retrieves. It\",\n",
       " 'get_dag_run_state_by_date',\n",
       " 'pool_list',\n",
       " 'pool_creator',\n",
       " 'delete_pool',\n",
       " 'create_container_group',\n",
       " 'get_instance_view',\n",
       " 'get_instance_events',\n",
       " 'get_container_logs',\n",
       " 'destroy_container_group',\n",
       " 'container_exists',\n",
       " 'init_args',\n",
       " 'druid_ingest',\n",
       " ' The provided function appears to be responsible for transferring a mail attachment from an IMAP server to an S3 bucket. Based on this functionality, a fitting name for the function could be:\\n```\\nimap_to_s3\\n```\\nThis name clearly conveys the purpose of the function, which is to transfer a mail attachment from IMAP to S3.',\n",
       " ' The provided function is a Redis PubSub sensor, which subscribes to one or more channels and waits for a message to arrive. The function name could be:\\n```\\nredis_pubsub_sensor\\n```\\nThis name clearly conveys the purpose of the function, which is to subscribe to Redis PubSub channels and wait for messages. The use of \"sensor\" in the name also implies that the function is designed to detect and respond to changes',\n",
       " 'dag_run_updater',\n",
       " 'find_dag_runs',\n",
       " 'find_task_instances',\n",
       " 'get_task_instance',\n",
       " 'get_previous_dag_run',\n",
       " 'get_previous_dag_run',\n",
       " 'dagrun_dependency_checker',\n",
       " ' The provided function is responsible for creating and managing task instances for a DAG. It appears to be a part of the Airflow scheduler.\\n\\nGiven this, a fitting name for the function could be something like `dag_task_instance_manager`. This name suggests that the function is responsible for managing the task instances for a DAG, and it also conveys the idea that the function is a key part of the Airflow scheduler.\\n\\nHere',\n",
       " 'get_latest_dag_run',\n",
       " 'handle_response',\n",
       " 'jenkins_build_request',\n",
       " 'jenkins_queue_poller',\n",
       " 'context_params',\n",
       " 'pre_exec_callbacks',\n",
       " 'post_execution_callbacks',\n",
       " 'conditionally_trigger',\n",
       " 'send_metric',\n",
       " 'get_metrics',\n",
       " 'notify',\n",
       " 'slack_token_fetcher',\n",
       " 'slack_message',\n",
       " 'slack_sender',\n",
       " 'dag_loader',\n",
       " 'dag_importer',\n",
       " 'zombie_killer',\n",
       " 'dag_loader',\n",
       " 'dag_loader',\n",
       " 'dagbag_stats',\n",
       " ' The function name `submit_jdbc_job` is a good fit for this function, as it accurately describes the purpose of the function, which is to submit a JDBC job to a Spark cluster.',\n",
       " 'date_adder',\n",
       " 'date_converter',\n",
       " 'register_macros',\n",
       " \" The provided function is a custom implementation of the `ls` method for the `HDFS` connection. The function takes a `context` parameter and returns a list of files that match the specified criteria.\\n\\nA fitting name for this function could be `hdfs_ls_filtered`, which reflects the purpose of the function and the fact that it filters the results of the `ls` method.\\n\\nHere's the suggested function name in triple backticks:\",\n",
       " 'file_exists',\n",
       " 'terminate_running_tasks',\n",
       " 'incrementer',\n",
       " 'executor',\n",
       " 'generate_task_command',\n",
       " 'airflow_run',\n",
       " 'get_task_instance_state',\n",
       " 'record_failure',\n",
       " 'fetch_task_instance',\n",
       " 'clear_xcom',\n",
       " 'dag_task_execution',\n",
       " 'check_downstream_complete',\n",
       " 'check_dependencies',\n",
       " 'exponential_backoff_delay',\n",
       " 'is_ready_for_retry',\n",
       " 'check_pool_open_slots',\n",
       " 'get_dag_run',\n",
       " 'run_task',\n",
       " 'task_runner',\n",
       " 'set_xcom',\n",
       " 'get_xcom_data',\n",
       " 'set_context',\n",
       " 'uploader',\n",
       " 'fetch_remote_log',\n",
       " 'check_blob_exists',\n",
       " 'get_log_file_content',\n",
       " ' The provided function is responsible for writing logs to a remote location, such as Azure Blob Storage. A fitting name for this function could be:\\n```\\nwrite_logs\\n```\\nThis name clearly conveys the purpose of the function, which is to write logs to a remote location. The use of \"write\" as a verb and \"logs\" as a noun is also appropriate, as the function is specifically designed to write logs to a remote location.',\n",
       " 'compute_client',\n",
       " 'start_instance',\n",
       " 'set_machine_type',\n",
       " 'get_instance_template',\n",
       " 'create_instance_template',\n",
       " 'get_instance_group_manager',\n",
       " 'update_instance_group_manager',\n",
       " 'wait_for_operation',\n",
       " 'bucket_exists',\n",
       " 'create_bucket',\n",
       " 'prefix_in_list',\n",
       " 'list_prefixes',\n",
       " 'object_list',\n",
       " 's3_object_exists',\n",
       " 's3_object_loader',\n",
       " 'get_key_body',\n",
       " 's3_select',\n",
       " 'get_wildcard_key_exists',\n",
       " 'wildcard_key_fetcher',\n",
       " 'upload_file',\n",
       " 'string_encoder',\n",
       " 'upload_bytes',\n",
       " 's3_uploader',\n",
       " 's3_copy',\n",
       " 'deleter',\n",
       " 'cassandra_query',\n",
       " 'json_writer',\n",
       " 'schema_builder',\n",
       " 'to_dict',\n",
       " 'field_adder',\n",
       " 'convert_map',\n",
       " 'send_email',\n",
       " 'speech_client',\n",
       " 'speech_recognizer',\n",
       " 'SparkSqlHook',\n",
       " 'file_handler',\n",
       " 'fetch_log',\n",
       " 'fetch_logs',\n",
       " ' The provided function is responsible for creating a file with a specific name and path, and setting its permissions.\\n\\nA fitting name for this function could be:\\n```\\ncreate_file\\n```\\nThis name clearly conveys the purpose of the function, which is to create a file with a specific name and path, and set its permissions.\\n\\nAlternatively, you could also consider the following names:\\n```\\nmake_file\\ncreate_file_with_perm',\n",
       " 'plugin_loader',\n",
       " 'plugin_validator',\n",
       " 'skipper',\n",
       " 'azure_adls_client',\n",
       " 'file_exists',\n",
       " 'uploader',\n",
       " 'downloader',\n",
       " 'walker',\n",
       " 'athena_query_runner',\n",
       " 'cancel_query',\n",
       " 'uncompress_file',\n",
       " 'sql_executor',\n",
       " 'json_writer',\n",
       " 'upload_to_gcs',\n",
       " 'to_float',\n",
       " 'cli_action_logger',\n",
       " 'cli_metrics',\n",
       " 'task_status_checker',\n",
       " 'cgroup_creator',\n",
       " 'delete_cgroup',\n",
       " 'cgroup_parser',\n",
       " 'hostname_resolver',\n",
       " 'databricks_request',\n",
       " 'salesforce_connection',\n",
       " 'query_all',\n",
       " 'describer',\n",
       " 'get_field_names',\n",
       " 'salesforce_query',\n",
       " 'to_timestamp',\n",
       " 'writer',\n",
       " 'fetch_remote_log',\n",
       " 'download_remote_log',\n",
       " 'writer',\n",
       " 'gs_url_parser',\n",
       " 'mongo_client_factory',\n",
       " 'get_collection',\n",
       " 'aggregator',\n",
       " 'finder',\n",
       " 'inserter',\n",
       " 'bulk_inserter',\n",
       " 'updater',\n",
       " 'updater',\n",
       " 'bulk_upsert',\n",
       " 'deleter',\n",
       " 'deleter',\n",
       " 'get_attachment_count_by_name',\n",
       " 'get_mail_attachments_by_name',\n",
       " 'download_attachments',\n",
       " 'find_attachments',\n",
       " 'get_filename_and_payload',\n",
       " 'firehose_put_record_batch',\n",
       " 'reschedule_eligibility_checker',\n",
       " 'send_email',\n",
       " 'email_sender',\n",
       " 'to_utc',\n",
       " 'blob_exists',\n",
       " 'blob_exists',\n",
       " 'upload_blob',\n",
       " 'upload_blob',\n",
       " 'download_blob',\n",
       " 'blob_reader',\n",
       " 'delete_blobs',\n",
       " 'mls_list',\n",
       " 'ftp_connection',\n",
       " 'file_list',\n",
       " 'file_list',\n",
       " 'download_file',\n",
       " 'upload_file',\n",
       " 'renamer',\n",
       " ' The provided function appears to be a method that retrieves the last modified time of a file on an FTP server. The function takes a path as an argument and returns a `datetime` object representing the last modified time of the file.\\n\\nGiven this information, a fitting name for the function could be `get_last_modified_time`. This name clearly conveys the purpose of the function and is easy to understand.\\n\\nHere is the function with the suggested',\n",
       " 'create_missing_dagruns',\n",
       " 'set_task_instance_state',\n",
       " 'set_dag_run_state',\n",
       " 'dag_runner',\n",
       " 'failed_task_instances',\n",
       " 'dag_runner',\n",
       " 'git_version',\n",
       " 'discord_webhook_sender',\n",
       " ' The provided function is a validation function for a dictionary that contains a set of fields with specifications. The function takes in a dictionary to validate, a validation specification, and a parent field name. It then validates the dictionary against the validation specification, checking for missing or invalid fields, and validating the values of the fields against the specified types and regular expressions.\\n\\nA fitting name for this function could be `validate_dictionary`.',\n",
       " 'validator',\n",
       " 'azure_file_service',\n",
       " 'check_exists',\n",
       " 'file_exists',\n",
       " 'list_files_and_directories',\n",
       " 'create_directory',\n",
       " 'downloader',\n",
       " 'downloader',\n",
       " 'uploader',\n",
       " 'upload_file',\n",
       " 'uploader',\n",
       " 'rotating_file_handler',\n",
       " 'writer',\n",
       " 'gs_url_parser',\n",
       " 'get_storage_client',\n",
       " 'copy_object',\n",
       " 'copy_object',\n",
       " 'download_blob',\n",
       " 'upload_file',\n",
       " 'blob_exists',\n",
       " 'object_verifier',\n",
       " 'deleter',\n",
       " 'blob_names_and_prefixes',\n",
       " 'get_object_size',\n",
       " 'crc32c_checksum',\n",
       " 'get_md5_hash',\n",
       " 'create_bucket',\n",
       " 'acl_setter',\n",
       " 'acl_setter',\n",
       " 'composer',\n",
       " 'find_min_index',\n",
       " 'status_message_changed',\n",
       " 'secondary_status_updates',\n",
       " 'compressor',\n",
       " 's3_bucket_creator',\n",
       " 's3_key_exists',\n",
       " 'get_logs_client',\n",
       " 'sagemaker_training_job',\n",
       " 'tuner',\n",
       " 'transform_job',\n",
       " 'endpoint_creator',\n",
       " 'log_monitor',\n",
       " 'monitor_sagemaker_job',\n",
       " 'sagemaker_training_monitor',\n",
       " 'dataflow_runner',\n",
       " 'gcs_downloader',\n",
       " 'alembic_migrator',\n",
       " 'alembic_migrator',\n",
       " 'deleter',\n",
       " 'create_instance',\n",
       " 'creator',\n",
       " 'deleter',\n",
       " 'cluster_manager',\n",
       " 'hive_query',\n",
       " 'hive_conf_adder',\n",
       " 'hive_query',\n",
       " 'hive_load_from_csv',\n",
       " 'create_table_and_load_data',\n",
       " 'hive_client_factory',\n",
       " 'is_partition_present',\n",
       " 'check_for_named_partition',\n",
       " 'get_table',\n",
       " 'get_tables_by_pattern',\n",
       " 'get_partition_info',\n",
       " ' The provided function is a Python function that takes three arguments: `part_specs`, `partition_key`, and `filter_map`. The function returns the maximum value of a list of partition keys from a list of dictionaries, where the dictionaries are the partition specifications and the partition key is the key in the dictionary that represents the partition.\\n\\nA fitting name for this function could be `get_max_partition_key`. This name clearly conveys the purpose',\n",
       " ' The provided function is a part of the Hive Metastore Hook in Apache Airflow. It retrieves the maximum value of a specified field from a Hive table. The function takes several parameters, including the schema, table name, field name, and a filter map.\\n\\nThe suggested name for this function is `get_max_partition`. This name clearly conveys the purpose of the function, which is to retrieve the maximum value of a specified field from a Hive',\n",
       " 'table_exists',\n",
       " 'hive_connect',\n",
       " 'query_results',\n",
       " ' The provided function is responsible for exporting the results of a Hive query to a CSV file. The function name could be `hive_to_csv` or `export_hive_to_csv`.\\n\\nHere is the suggested function name in triple backticks:\\n```\\nhive_to_csv\\n```',\n",
       " 'query_results',\n",
       " 'hql_to_dataframe',\n",
       " 'search_client',\n",
       " 'create_product_set',\n",
       " 'get_product_set',\n",
       " 'update_product_set',\n",
       " 'delete_product_set',\n",
       " 'create_product',\n",
       " 'get_product',\n",
       " 'update_product',\n",
       " 'delete_product',\n",
       " 'create_reference_image',\n",
       " 'delete_reference_image',\n",
       " 'add_product_to_product_set',\n",
       " 'image_annotator',\n",
       " 'safe_search_detection',\n",
       " ' The function name `get_dingding_token` would be a fitting name for this function.',\n",
       " 'json_builder',\n",
       " 'dingding_message_sender',\n",
       " ' ```\\n\\nThe provided function is a custom implementation of a log reader that reads logs from an Elasticsearch index. The function takes in the following parameters:\\n\\n* `ti`: The task instance object\\n* `try_number`: The current try number of the task\\n* `metadata`: A dictionary containing metadata about the log reader, including the current offset and whether the end of the log has been reached\\n\\nThe function first checks if the `metadata` dictionary is empty. If',\n",
       " 'fetch_logs',\n",
       " 'parameterize',\n",
       " 'escaper',\n",
       " 'string_to_bq_type',\n",
       " 'type_checker',\n",
       " 'bigquery_connection',\n",
       " 'bigquery_client',\n",
       " 'query_gbq',\n",
       " 'get_table',\n",
       " 'create_table',\n",
       " \" The provided function is a method for creating an external table in Google BigQuery. The function takes several parameters, including the name of the external table, the schema of the table, the source URIs of the data, and various other options for configuring the table.\\n\\nBased on the parameters and the function's purpose, a fitting name for this function could be `create_external_table`. This name clearly conveys the function's purpose and makes it easy\",\n",
       " ' The provided function is a method for patching a BigQuery table. It takes several parameters, including the project ID, dataset ID, table ID, and some optional parameters such as description, expiration time, external data configuration, friendly name, labels, schema, time partitioning, and view.\\n\\nThe function first checks if the project ID is provided, and if not, it uses the default project ID from the service object. It then creates a dictionary called `table_resource`',\n",
       " 'query_runner',\n",
       " 'bq_export',\n",
       " 'copy_tables',\n",
       " 'loader',\n",
       " \" The provided function is responsible for submitting a BigQuery job and polling its status until the job is completed. The function takes a `configuration` parameter, which is a dictionary containing the job configuration.\\n\\nBased on the function's behavior, a fitting name for the function could be `submit_and_poll_job`. This name suggests that the function will submit a BigQuery job and then poll its status until the job is completed.\\n\\nHere is the function\",\n",
       " 'cancel_job',\n",
       " 'get_table_schema',\n",
       " 'retriever',\n",
       " 'deleter',\n",
       " 'creator',\n",
       " 'grant_view_access',\n",
       " 'create_dataset',\n",
       " 'delete_dataset',\n",
       " 'get_dataset_resource',\n",
       " 'dataset_fetcher',\n",
       " 'bq_insert_rows',\n",
       " 'executor',\n",
       " 'sequencer',\n",
       " 'next_row',\n",
       " 'fetch_all',\n",
       " 'fetch_all',\n",
       " 'get_asset_url',\n",
       " 'fetch_data',\n",
       " 'create_temp_file',\n",
       " 'schema_builder',\n",
       " 'to_timestamp',\n",
       " 'mkdir',\n",
       " 'create_queue',\n",
       " 'send_message',\n",
       " 'airflow_plugin_loader',\n",
       " 'runner',\n",
       " 'remover',\n",
       " 'nvd3_options_parser',\n",
       " 'add_serie',\n",
       " 'builder',\n",
       " 'builder',\n",
       " 'html_header_builder',\n",
       " 'embedder',\n",
       " \" ```\\n\\ndef generate_tooltip_condition_string(self):\\n    self.jschart = ''\\n    if self.tooltip_condition_string == '':\\n        self.tooltip_condition_string = 'var y = String(graph.point.y);\\\\n'\\n    self.series_js = json.dumps(self.series)\\n```\\n\\nThis function is named `generate_tooltip_condition_string`\",\n",
       " 'get_axis_config',\n",
       " 'axis_config',\n",
       " 'chartbuilder',\n",
       " 'sqlite_connection',\n",
       " 'log_request',\n",
       " 'gzip_response',\n",
       " 'dag_access_decorator',\n",
       " 'get_latest_dag_run',\n",
       " 'create_dagrun',\n",
       " ' The provided function is a method that sends a message to an SQS queue using the `SQSHook` class. The function takes in a `context` parameter and uses it to create an instance of the `SQSHook` class. The function then calls the `send_message` method on the `SQSHook` instance, passing in the necessary parameters to send the message to the SQS queue.\\n\\nGiven this information, a fitting name',\n",
       " 'pagination',\n",
       " 'json_response',\n",
       " 'open_file',\n",
       " 'hasher',\n",
       " 'video_intelligence_client',\n",
       " 'video_annotator',\n",
       " 'get_api_key',\n",
       " 'opsgenie_session',\n",
       " 'alert_creator',\n",
       " 'bash_command_runner',\n",
       " 'payload_builder',\n",
       " 'opsgenie_alert_hook',\n",
       " 'athena_client',\n",
       " 'query_executor',\n",
       " 'get_query_execution_state',\n",
       " 'fetch_query_results',\n",
       " 'wait_for_query_completion',\n",
       " 'sftp_connection',\n",
       " 'file_listing',\n",
       " 'file_list',\n",
       " 'mkdir',\n",
       " 'retriever',\n",
       " 'uploader',\n",
       " 'pause_on_rate_limit',\n",
       " 'fetcher',\n",
       " 'get_partitions',\n",
       " 'has_partition',\n",
       " 'get_table',\n",
       " 'get_table_location',\n",
       " 'get_cluster_status',\n",
       " 'terminate_cluster',\n",
       " 'get_latest_snapshots',\n",
       " 'restore_cluster_from_snapshot',\n",
       " 'create_cluster_snapshot',\n",
       " 'slack_api_caller',\n",
       " 'add_volume',\n",
       " 'add_volume_mount',\n",
       " 'create_job_flow',\n",
       " 'file_size_filter',\n",
       " 'filter_by_extension',\n",
       " 's3_uploader',\n",
       " 'json_dumper',\n",
       " 'get_pool',\n",
       " 'create_pool',\n",
       " 'delete_pool',\n",
       " 'json_parser',\n",
       " 'wait_for_operation',\n",
       " 'get_operation',\n",
       " 'cluster_label_updater',\n",
       " ' The provided function appears to be a method for deleting a cluster in a Google Kubernetes Engine (GKE) project. Based on the function signature, the suggested name for the function could be:\\n```\\ndelete_cluster\\n```\\nThis name suggests that the function is responsible for deleting a cluster in a GKE project, and it follows the standard naming convention for Python methods.',\n",
       " 'create_cluster',\n",
       " 'cluster_getter',\n",
       " 'discord_webhook_endpoint',\n",
       " ' The provided function is a method that generates a JSON payload for a Discord message. It takes the following parameters:\\n\\n* `self`: The instance of the class that the method is being called on.\\n* `username`: The username of the Discord user who is sending the message.\\n* `avatar_url`: The URL of the avatar of the Discord user who is sending the message.\\n* `tts`: Whether the message should be sent as a',\n",
       " 'discord_notifier',\n",
       " 'encrypter',\n",
       " ' The provided function is a wrapper around the `subprocess.Popen` function, which is used to execute a command in a separate process. The function takes a command string and a set of keyword arguments, and returns the output of the command as a list of lines.\\n\\nA fitting name for this function could be `execute_command`, which reflects the purpose of the function and makes it clear what it does.\\n\\nHere is the suggested function name in triple backticks',\n",
       " 'importer',\n",
       " 'importer',\n",
       " 'export_table',\n",
       " 'text_to_speech_client',\n",
       " 'synthesize',\n",
       " 'uploader',\n",
       " 'get_log_contents',\n",
       " ' The provided function is responsible for writing logs to an S3 bucket. A fitting name for this function could be:\\n```\\ns3_log_writer\\n```\\nThis name clearly conveys the purpose of the function, which is to write logs to an S3 bucket. The use of \"s3\" in the name also makes it clear that the function is related to Amazon S3.',\n",
       " 'git_sync_init_container',\n",
       " 'airflow_env',\n",
       " 'secret_loader',\n",
       " 'kube_security_context',\n",
       " 'cancel_qubole_command',\n",
       " \" The provided function is responsible for retrieving the results of a command execution and writing them to a file. The function takes several parameters, including a file pointer `fp`, an optional delimiter `delim`, and a flag indicating whether to fetch the results or not `fetch`.\\n\\nBased on the function's purpose and the parameters it takes, a fitting name for this function could be `write_results_to_file`. This name clearly conveys the function's purpose\",\n",
       " 'get_log_id',\n",
       " 'get_jobs_id_from_xcom',\n",
       " 'qubole_command_url',\n",
       " 'heartbeat_job',\n",
       " 'reset_task_instances',\n",
       " ' The provided function is a helper function that is used to process a file in a separate process. The function takes several arguments, including a result queue, a file path, a list of pickled DAGs, a list of white-listed DAG IDs, a thread name, and a list of zombie processes.\\n\\nGiven this information, a fitting name for the function could be `process_file_in_process`. This name clearly conveys the purpose',\n",
       " 'init_processor',\n",
       " 'terminator',\n",
       " ' The provided function is a part of the `airflow.utils.process_utils` module in Apache Airflow. The function is called `is_process_done` and it checks if a process has finished or not.\\n\\nThe function takes no arguments and returns a boolean value indicating whether the process has finished or not.\\n\\nThe function first checks if the process is None, and if so, it raises an AirflowException. This is because the function is not meant',\n",
       " 'signal_handler',\n",
       " 'sla_checker',\n",
       " 'dagbag_importer',\n",
       " 'scheduler',\n",
       " 'scheduler',\n",
       " 'set_task_instances_to_state_if_dag_run_not_running',\n",
       " 'task_concurrency_counter',\n",
       " 'task_scheduler',\n",
       " 'task_instance_state_changer',\n",
       " 'task_instance_executor',\n",
       " 'task_state_changer',\n",
       " 'schedule_queued_tasks',\n",
       " 'dag_processor',\n",
       " 'task_state_updater',\n",
       " 'scheduler_loop',\n",
       " 'dag_loader',\n",
       " 'task_instance_status_updater',\n",
       " 'task_state_consistency_checker',\n",
       " 'backfill_dag',\n",
       " 'task_scheduler',\n",
       " \"0:\\n            self.log.debug('All tasks are not ready, sleeping for %s seconds',\\n                self.sleep_time)\\n            time.sleep(self.sleep_time)\\n        else:\\n            self.log.debug('Sleeping for %s seconds', self.sleep_time)\\n            time.sleep(self.sleep_time)\\n\\n    self.log.debug('Finished backfilling %s', self.\",\n",
       " 'backfill_task_instances',\n",
       " 'updater',\n",
       " ' The provided function is a method of the `BackfillJob` class, and it appears to be responsible for executing a backfill operation on a DAG. The function takes a `session` parameter, which is an instance of a SQLAlchemy session, and it also takes a `self` parameter, which is an instance of the `BackfillJob` class.\\n\\nThe function first initializes a few variables, including `ti_status`, `start_date`, and',\n",
       " 'check_runner_state',\n",
       " 'project_client',\n",
       " 'get_instance',\n",
       " \" The provided function is a method that creates a new instance of a Google Cloud Spanner database. The function takes several parameters, including the project ID, instance ID, configuration name, node count, display name, and a function that performs the actual instance creation.\\n\\nBased on the function's purpose and the parameters it takes, a fitting name for the function could be:\\n```\\ncreate_instance\\n```\\nThis name clearly conveys the function's purpose and the\",\n",
       " 'creator',\n",
       " 'updater',\n",
       " 'deleter',\n",
       " 'get_database',\n",
       " 'create_database',\n",
       " 'ddl_updater',\n",
       " 'delete_database',\n",
       " 'transaction_executor',\n",
       " 'has_attachment',\n",
       " 'merger',\n",
       " 'session_manager',\n",
       " 'is_table_exists',\n",
       " 'exists',\n",
       " 'spark_submit',\n",
       " 'poll_driver_status',\n",
       " 'submit_spark_job',\n",
       " 'parse_spark_submit_output',\n",
       " 'get_driver_status',\n",
       " 'poll_driver_status',\n",
       " 'spark_killer',\n",
       " ' The function you provided is a factory method that returns an instance of a task runner based on the value of the `_TASK_RUNNER` environment variable.\\n\\nHere are a few suggestions for a fitting name for this function:\\n\\n1. `get_task_runner`\\n2. `create_task_runner`\\n3. `task_runner_factory`\\n4. `task_runner_creator`\\n5. `task_runner_',\n",
       " 'job_completion_waiter',\n",
       " 'mysql_query',\n",
       " 'file_writer',\n",
       " 'writer_initializer',\n",
       " 'schema_file_to_upload',\n",
       " 'upload_to_gcs',\n",
       " 'convert_row',\n",
       " 'schema_parser',\n",
       " 'mysql_type_to_python_type',\n",
       " 'authenticator',\n",
       " 'sqoop_runner',\n",
       " 'lineage_tracker',\n",
       " 'lineage_inlets_outlets',\n",
       " 'json_parser',\n",
       " 'date_range',\n",
       " 'rounder',\n",
       " 'time_unit_converter',\n",
       " 'converter',\n",
       " 'get_previous_date',\n",
       " 'get_dag_runs',\n",
       " 'init_role_permissions',\n",
       " 'deleter',\n",
       " 'get_user_roles',\n",
       " 'permissions_views',\n",
       " 'get_user_permissions',\n",
       " 'check_view_access',\n",
       " 'has_role',\n",
       " 'has_permission',\n",
       " 'clean_faulty_perms',\n",
       " 'add_permission_view_menu',\n",
       " \" The provided function appears to be responsible for updating the permissions for a user in a specific role. The function takes a session object as an argument and performs a number of queries on the database to retrieve the necessary data.\\n\\nGiven this, it seems like a fitting name for the function could be something like `update_user_permissions`. This name clearly conveys the purpose of the function and makes it easy to understand what it does.\\n\\nHere's the suggested function\",\n",
       " 'grant_admin_permissions',\n",
       " 'sync_user_roles',\n",
       " 'dag_permission_adder',\n",
       " 'dag_perm_manager',\n",
       " 'dag_vm_perm_merger',\n",
       " 'fernet_factory',\n",
       " 'poke_for_partition',\n",
       " 'glue_catalog_hook',\n",
       " 'sqs_message_deleter',\n",
       " ' The provided function is a wrapper around the `json.dump()` function, and it is responsible for creating a temporary file and writing the configuration dictionary to it. The function takes an optional `chmod` parameter, which is used to set the permissions on the temporary file.\\n\\nGiven this information, a fitting name for the function could be `create_temp_config_file()`. This name clearly conveys the purpose of the function, which is to create a temporary configuration file',\n",
       " 'hdfs_client_factory',\n",
       " ' The provided function is a custom implementation of the `HdfsHook` class in Apache Airflow. The function is responsible for establishing a connection to a HDFS cluster and returning a client object that can be used to perform HDFS operations.\\n\\nThe function name `x` is not very descriptive and does not accurately reflect the purpose of the function. A more fitting name for the function could be `get_hdfs_client`, which clearly communicates the',\n",
       " 'is_file_exists',\n",
       " 'upload_file',\n",
       " 'pinot_broker_connection',\n",
       " 'get_connection_url',\n",
       " 'query_executor',\n",
       " 'fetch_one',\n",
       " 'truncator',\n",
       " 'sanitize_text',\n",
       " 'set_xcom',\n",
       " 'get_xcom_value',\n",
       " 'get_task_instances_by_execution_date',\n",
       " 'date_parser',\n",
       " 'time_components',\n",
       " 'redis_client_factory',\n",
       " 'oracle_connect',\n",
       " 'loader',\n",
       " 'bulk_insert',\n",
       " 'db_connector',\n",
       " 'sql_reader',\n",
       " 'query_executor',\n",
       " 'fetch_one',\n",
       " 'executor',\n",
       " 'set_autocommit',\n",
       " 'loader',\n",
       " 'to_string',\n",
       " 'scheduler_status_checker',\n",
       " 'get_extra_link_url',\n",
       " 'get_active_dags',\n",
       " 'get_active_dag_count',\n",
       " 'cloudant_client',\n",
       " 'slack_webhook_sender',\n",
       " 'get_credentials',\n",
       " 'authenticator',\n",
       " 'get_extra_attribute',\n",
       " 'google_api_call',\n",
       " 'gcp_project_id_wrapper',\n",
       " 'status_list',\n",
       " 'purger',\n",
       " 'spark_sql_client',\n",
       " 'spark_sql_execute',\n",
       " 'vgg11_bn_pretrained',\n",
       " 'vgg13_pretrained',\n",
       " 'alexnet',\n",
       " 'densenet121',\n",
       " 'to_tensor',\n",
       " 'importer',\n",
       " 'normalizer',\n",
       " 'resizer',\n",
       " 'padder',\n",
       " 'cropper',\n",
       " 'cropper',\n",
       " 'flipper',\n",
       " ' The provided function appears to be a implementation of the Least Squares method for finding the best-fit line through a set of data points. The function takes two arguments, `startpoints` and `endpoints`, which are both lists of 2D points. The function returns a list of 2D points that represent the best-fit line.\\n\\nGiven this information, a fitting name for the function could be:\\n```\\nleast_squares_line',\n",
       " 'perspective_transform',\n",
       " 'flipper',\n",
       " 'cropper',\n",
       " 'five_crop_with_flip',\n",
       " 'brighter',\n",
       " 'contrast_enhancer',\n",
       " 'saturator',\n",
       " 'hue_adder',\n",
       " 'gamma_correction',\n",
       " 'rotate',\n",
       " 'affine_transform',\n",
       " 'grayscale',\n",
       " 'tensor_grid',\n",
       " ' The provided function is a utility function for visualizing PyTorch tensors as images. It is called `save_image`.',\n",
       " 'directory_scanner',\n",
       " ' The provided function is a data preprocessing function that takes in a directory of images, a file extension, and a maximum number of patches to extract, and returns a PyTorch tensor of shape `(n, 64, 64, 3)` containing the extracted patches.\\n\\nA fitting name for this function could be `patch_extractor`. This name clearly conveys the purpose of the function, which is to extract patches from images. The use',\n",
       " 'label_loader',\n",
       " 'read_matches',\n",
       " 'conv1x1',\n",
       " 'accuracy',\n",
       " 'master_print',\n",
       " 'updater',\n",
       " 'squeezenet1_1',\n",
       " 'mkdir',\n",
       " 'download_file',\n",
       " 'directory_list',\n",
       " 'file_finder',\n",
       " 'download_file',\n",
       " 'cropper',\n",
       " 'distortion_adder',\n",
       " 'random_crop',\n",
       " 'color_jitter',\n",
       " 'augmenter',\n",
       " 'inception_v3',\n",
       " 'download_tarball',\n",
       " 'googlenet_pretrained',\n",
       " ' The provided function is responsible for downloading and processing the CIFAR-10 dataset. A fitting name for this function could be:\\n```\\ndownload_and_process_cifar10\\n```\\nThis name clearly conveys the purpose of the function, which is to download and process the CIFAR-10 dataset.',\n",
       " 'download_and_extract_emnist',\n",
       " 'fetcher',\n",
       " 'get_theme_name',\n",
       " 'search_results',\n",
       " 'query_builder',\n",
       " 'preferences_manager',\n",
       " 'search_query',\n",
       " 'duden_search_results',\n",
       " 'theme_selector',\n",
       " 'suggester',\n",
       " 'currency_converter',\n",
       " 'custom_gradient',\n",
       " 'autodiff',\n",
       " 'tfp_independent_normal',\n",
       " 'treatment_model',\n",
       " 'mcmc_estimate_parameters',\n",
       " 'docstring_substituter',\n",
       " 'distribution_name',\n",
       " 'random_variable',\n",
       " 'distributor',\n",
       " 'distribution_factory',\n",
       " 'shape',\n",
       " 'one_step_predictive',\n",
       " 'forecast',\n",
       " 'max_or_mask',\n",
       " 'logsumexp',\n",
       " 'finite_assert',\n",
       " 'assert_rank_at_most',\n",
       " 'event_size',\n",
       " 'eval_all_one_hot',\n",
       " 'kldivergence_loss',\n",
       " \" The provided function is a deserialization function that takes an identifier and returns a deserialized object.\\n\\nA fitting name for this function could be `deserialize`.\\n\\nHere's the function with the suggested name:\\n```\\ndef deserialize(identifier):\\n    if identifier is None:\\n        return None\\n    if isinstance(identifier, six.string_types):\\n        identifier = str(identifier)\\n        return _deserialize(identifier\",\n",
       " 'distribution_lambda',\n",
       " \" The provided function is a custom implementation of the `MultivariateNormalTriL` distribution in TensorFlow Probability. The function takes in a set of parameters, an event size, and a few other optional arguments.\\n\\nThe suggested name for this function is `MultivariateNormalTriL`. This name is a good fit because it accurately describes the function's purpose, which is to create a multivariate normal distribution with a lower triangular matrix of covar\",\n",
       " 'multivariate_normal_tri_l_params_size',\n",
       " 'onehot_categorical',\n",
       " 'categorical_mixture_of_one_hot_categorical',\n",
       " 'categorical_mixture_of_one_hot_categorical_params_size',\n",
       " 'independent_bernoulli',\n",
       " 'independent_bernoulli',\n",
       " 'independent_logistic',\n",
       " 'independent_normal_params_size',\n",
       " 'independent_poisson',\n",
       " 'mixture_same_family',\n",
       " ' The provided function is a custom implementation of a mixture of same-family distributions. The function takes in three parameters:\\n\\n* `num_components`: The number of components in the mixture.\\n* `component_params_size`: The size of the parameters for each component.\\n* `name`: The name of the function.\\n\\nThe function returns the sum of the number of components and the product of the number of components and the size of the parameters for each component.\\n',\n",
       " 'mixture_normal_params_size',\n",
       " 'mixture_logistic',\n",
       " 'mixture_logistic_params',\n",
       " 'interceptor_stack_pop',\n",
       " 'interceptor',\n",
       " 'tape',\n",
       " 'logistic_regression',\n",
       " ' The provided function is a visualization tool for plotting the decision boundary of a linear classifier. It takes in several parameters, including the features, labels, true weights and bias, and candidate weights and biases. The function returns a plot of the decision boundary, along with the true separator (if provided) and the candidate separators.\\n\\nA fitting name for this function could be `plot_decision_boundary`. This name clearly conveys the purpose of the function',\n",
       " 'batch_generator',\n",
       " \" ```\\n  def validate_map_values(map_values, validate_args):\\n    assertions = []\\n    message = 'Rank of map_values must be 1.'\\n    if tensorshape_util.rank(map_values.shape) is not None:\\n        if tensorshape_util.rank(map_values.shape) != 1:\\n            raise ValueError(message)\\n    elif validate_args:\\n        assertions\",\n",
       " ' The provided function is a custom implementation of a Markov chain Monte Carlo (MCMC) algorithm. The function takes in a `state`, a `fn`, a `num_steps`, and a `trace_fn` as input, and returns a tuple of the final state and the full trace of the MCMC algorithm.\\n\\nThe function first calls the `fn` on the `state` to obtain the next state. It then calls the `trace_fn` on the `',\n",
       " ' The provided function is a wrapper function that takes a transition operator and some arguments, and it calls the transition operator with the arguments. The function is named `x` and it is not a good name for this function.\\n\\nA better name for this function could be `call_transition_operator`, which clearly describes what the function does. This name is more descriptive and easier to understand than the current name.\\n\\nHere is the suggested function name:\\n```\\ncall_transition',\n",
       " 'gradient_tape',\n",
       " 'broadcaster',\n",
       " 'transformer',\n",
       " 'leapfrog_step',\n",
       " 'propose_state',\n",
       " ' The provided function is a implementation of the HMC algorithm for sampling from a target distribution. The function takes in several parameters, including the current state of the Markov chain, the target log probability function, the step size, the number of leapfrog steps, and the momentum. The function then performs a series of leapfrog steps to propagate the state of the Markov chain forward in time, and uses the Metropolis-Hastings algorithm to determine whether to accept',\n",
       " 'adaptive_control',\n",
       " 'conv_output_length',\n",
       " ' The provided function is a method of the `_ConvVariational` class in TensorFlow Probability, and it appears to be responsible for serializing the configuration of the layer.\\n\\nGiven this, a fitting name for the function could be `serialize_config`. This name suggests that the function is responsible for serializing the configuration of the layer, which is a common task in machine learning.\\n\\nHere is the suggested function name in triple backticks: ```',\n",
       " 'configure_functions',\n",
       " 'flipout_conv',\n",
       " 'tensorify',\n",
       " 'perturb_diagonal',\n",
       " 'random_walk_normal_fn',\n",
       " 'random_walk_uniform_fn',\n",
       " 'kl_independent',\n",
       " 'get_event_dim',\n",
       " 'expand_dims',\n",
       " 'entropy_lower_bound',\n",
       " 'cat_probs_selector',\n",
       " 'validate_equal_last_dim',\n",
       " 'tf_import_checker',\n",
       " 'logistic_regression',\n",
       " 'covtype_preprocessing',\n",
       " ' The provided function is a Kullback-Leibler divergence between two Dirichlet distributions. A fitting name for this function could be:\\n```\\nkl_dirichlet_dirichlet\\n```\\nThis name clearly indicates that the function computes the Kullback-Leibler divergence between two Dirichlet distributions.',\n",
       " 'concentration_parameter_checker',\n",
       " 'sample_with_positive_sum',\n",
       " 'auto_correlation',\n",
       " ' The provided function is a custom implementation of the Cholesky decomposition of a covariance matrix. The function takes in a tensor `x` and returns the Cholesky decomposition of the covariance matrix of `x`.\\n\\nA fitting name for this function could be `cholesky_decomposition`. This name clearly conveys the purpose of the function, which is to compute the Cholesky decomposition of a covariance matrix.\\n\\nTherefore, the',\n",
       " 'covariance',\n",
       " 'correlation',\n",
       " 'stddev',\n",
       " 'variance',\n",
       " ' The provided function is a custom implementation of a function that adds a scalar value to a tensor. The function takes a tensor as input and returns a tensor with the same shape and dtype as the input, but with the scalar value added to each element.\\n\\nThe function name that I would suggest is `add_scalar`. This name is descriptive and clearly conveys the purpose of the function. It also follows the standard naming conventions for TensorFlow functions.\\n\\nHere',\n",
       " 'axis_adjuster',\n",
       " ' The provided function is a custom implementation of the `squeeze` function in TensorFlow. The function takes two arguments: `x` and `axis`.\\n\\nThe function first converts `x` to a tensor using the `tf.convert_to_tensor` function. It then checks if `axis` is `None`. If it is, the function returns the result of calling `tf.squeeze` on `x` with `axis=None`.',\n",
       " 'kl_normal_normal',\n",
       " 'standardizer',\n",
       " 'reconstructor',\n",
       " 'AutoregressiveLinearOperator',\n",
       " 'slope_model',\n",
       " 'halton_sequence',\n",
       " 'halton_sequence',\n",
       " ' The provided function is a custom implementation of a Markov Chain Monte Carlo (MCMC) algorithm. The function takes in three parameters: `num_results`, `dims`, and `seed`.\\n\\nThe function first generates a sequence of random numbers using the `tf.range` function, with the number of elements equal to `num_results`. It then creates a `SeedStream` object using the `distributions.SeedStream` function, with the `seed',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"accuracy\"]\n",
    "results[\"predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\\\n\\\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\\\'t know the answer to a question, please don\\\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\\\n' + content.strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.default_chat_template"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
