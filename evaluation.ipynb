{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis of finetune model on function name prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/workspace/s3/hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# We will only use 1000 samples from test set, to reduce inference time \n",
    "test_dst = load_dataset(\"hynky/code_search_net_python_func_names\")[\"test\"].select(range(1000))\n",
    "# Sort by length for faster inference\n",
    "test_dst = test_dst.map(lambda x: {\"len\": len(x[\"source_code\"])})\n",
    "test_dst = test_dst.sort(\"len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "def get_fc_name_tokens(fc_name):\n",
    "    \"\"\"\n",
    "    This function parses the function name and returns its \"tokens\",\n",
    "    splitting on underscores and camel case.\n",
    "    \"\"\"\n",
    "    # Split on underscores\n",
    "    parts = fc_name.split('_')\n",
    "\n",
    "    # Split camel case parts\n",
    "    tokens = []\n",
    "    for part in parts:\n",
    "        tokens.extend(re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?=[A-Z]|$)', part))\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_precision(expected, predicted):\n",
    "    matching_tokens = np.sum(np.isin(predicted, expected))\n",
    "    precision = matching_tokens / len(predicted) if predicted else 0\n",
    "    return precision\n",
    "\n",
    "def calculate_recall(expected, predicted):\n",
    "    matching_tokens = np.sum(np.isin(predicted, expected))\n",
    "    recall = matching_tokens / len(expected) if expected else 0\n",
    "    return recall\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall else 0\n",
    "    return f1\n",
    "\n",
    "def calculate_average_metric(expected_tokens, predicted_tokens, metric_func):\n",
    "    total_metric = np.sum([metric_func(expected, predicted) for expected, predicted in zip(expected_tokens, predicted_tokens)])\n",
    "    average_metric = total_metric / len(expected_tokens)\n",
    "    return average_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def predict_with_model(model, tokenizer, dataset, batch_size: int, max_length: int = 2048, max_gen_tokens=100):\n",
    "    padding = True if batch_size > 1 else \"do_not_pad\"\n",
    "    predictions = []\n",
    "    for batch_start in tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch = dataset.select(range(batch_start, min(batch_start + batch_size, len(dataset))))\n",
    "        tokenized = batch.map(lambda x: tokenizer(x[\"text\"], padding=padding, \n",
    "                                                    add_special_tokens=False, max_length=max_length,\n",
    "                                                    truncation=True, verbose=False), batched=True)\n",
    "        inputs = torch.tensor(tokenized[\"input_ids\"]).to(\"cuda\")\n",
    "        attention_mask = torch.tensor(tokenized[\"attention_mask\"]).to(\"cuda\")\n",
    "        generated_tokens = model.generate(inputs=inputs,\n",
    "                                attention_mask=attention_mask,\n",
    "                                max_new_tokens=max_gen_tokens)\n",
    "\n",
    "        generated_tokens = generated_tokens.to(\"cpu\")\n",
    "        predicted_names = tokenizer.batch_decode(generated_tokens[:, inputs.shape[1]:], skip_special_tokens=True)\n",
    "        predictions.extend(predicted_names)\n",
    "\n",
    "        # Free cuda memory\n",
    "        del generated_tokens\n",
    "        del inputs\n",
    "        del attention_mask\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def calculate_accuracy(true_tokens, predicted_tokens, tokenizer):\n",
    "    # Convert tokens to text\n",
    "    true_texts = [tokenizer.decode(tokens) for tokens in true_tokens]\n",
    "    predicted_texts = [tokenizer.decode(tokens) for tokens in predicted_tokens]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_texts, predicted_texts)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_function_name(text):\n",
    "    match = re.search(r\"```(.*)```\", text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    match_simple = re.search(r\"`(.*)`\", text)\n",
    "    if match_simple:\n",
    "        return match_simple.group(1)\n",
    "    return text\n",
    "\n",
    "def evaluate_with_model(model, tokenizer, dataset, batch_size: int, max_length: int = 2048, max_gen_tokens=100, transform_predictions=lambda x: x):\n",
    "    # model = None\n",
    "    predictions = predict_with_model(model, tokenizer, dataset, batch_size, max_length, max_gen_tokens)\n",
    "    predictions = transform_predictions(predictions)\n",
    "    expected_tokens = [get_fc_name_tokens(x) for x in dataset[\"function_name\"]]\n",
    "    predictions_tokens = [get_fc_name_tokens(x) for x in predictions]\n",
    "\n",
    "    precision = [calculate_precision(exp, pred) for exp,pred in zip(expected_tokens, predictions_tokens)]\n",
    "    recall = [calculate_recall(exp, pred) for exp,pred in zip(expected_tokens, predictions_tokens)]\n",
    "    f1 = [calculate_f1(prec, recall) for prec,recall in zip(precision, recall)]\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(predictions, dataset[\"function_name\"]),\n",
    "        \"recall\":  np.sum(recall) / len(predictions_tokens),\n",
    "        \"precision\": np.sum(precision) / len(predictions_tokens),\n",
    "        \"f1\": np.sum(f1) / len(predictions_tokens),\n",
    "        \"predictions\": predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple prompting without finetune_model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd73b353a644c83af9da29efbadd139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Since we are able to bit the model in memory, we don't have to go for peft\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, use_flash_attention_2=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "def apply_template(x, tokenizer):\n",
    "    return {\"text\": tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"system\", \"content\": \"Generate a fitting name for the provided function. Place the suggested function name inside tripplet backtics. E.g ```adder```\"},\n",
    "         {\"role\": \"user\", \"content\": x[\"source_code\"]}],\n",
    "        tokenize=False)\n",
    "    }\n",
    "\n",
    "dataset = test_dst.map(apply_template, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "results_base = evaluate_with_model(model, tokenizer, dataset, 3, transform_predictions=lambda x: [extract_function_name(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "del model\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuned LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ab0f3551c041b795cc0aff820a3d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d7c3981832461fadcf63876e97040f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"hynky/codellama-7b-sft-lora-func-names\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Since we are able to bit the model in memory, we don't have to go for peft\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, use_flash_attention_2=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "def apply_template(x, tokenizer):\n",
    "    return {\"text\": tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"system\", \"content\": f\"Given the source code of a python function, suggest a fitting name for the function.\"},\n",
    "        {\"role\": \"user\", \"content\": x[\"source_code\"]}],\n",
    "        tokenize=False)\n",
    "    }\n",
    "\n",
    "dataset = test_dst.map(apply_template, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "# I am not sure why the model adds space at the end of each function name\n",
    "results_lora = evaluate_with_model(model, tokenizer, dataset, 3, transform_predictions=lambda preds: [pred.strip() for pred in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune QLora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bcf693ee944ec08686f3a97aa876bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from peft.auto import AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "model_name = \"hynky/codellama-7b-sft-lora-func-names-4bit\"\n",
    "# Load in 4-bit as was trained\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, load_in_4bit=True, device_map=\"auto\",\n",
    "                                             use_flash_attention_2=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(x, tokenizer):\n",
    "    return {\"text\": tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"system\", \"content\": f\"Given the source code of a python function, suggest a fitting name for the function.\"},\n",
    "        {\"role\": \"user\", \"content\": x[\"source_code\"]}],\n",
    "        tokenize=False)\n",
    "    }\n",
    "dataset = test_dst.map(apply_template, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "# I am not sure why the model adds space at the end of each function name\n",
    "results_qlora = evaluate_with_model(model, tokenizer, dataset, 6, transform_predictions=lambda preds: [pred.strip() for pred in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base</th>\n",
       "      <th>LoRA</th>\n",
       "      <th>Q-LoRA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.346998</td>\n",
       "      <td>0.619457</td>\n",
       "      <td>0.606014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.323036</td>\n",
       "      <td>0.633829</td>\n",
       "      <td>0.628983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.320728</td>\n",
       "      <td>0.614021</td>\n",
       "      <td>0.603586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictions</th>\n",
       "      <td>[set_context, timestamp, mkdir, timestamp, vol...</td>\n",
       "      <td>[set_context, now, mkdir, epoch, add_volume, g...</td>\n",
       "      <td>[set_context, now, mkdir, epoch, add_volume, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          Base  \\\n",
       "accuracy                                                  0.13   \n",
       "recall                                                0.346998   \n",
       "precision                                             0.323036   \n",
       "f1                                                    0.320728   \n",
       "predictions  [set_context, timestamp, mkdir, timestamp, vol...   \n",
       "\n",
       "                                                          LoRA  \\\n",
       "accuracy                                                 0.394   \n",
       "recall                                                0.619457   \n",
       "precision                                             0.633829   \n",
       "f1                                                    0.614021   \n",
       "predictions  [set_context, now, mkdir, epoch, add_volume, g...   \n",
       "\n",
       "                                                        Q-LoRA  \n",
       "accuracy                                                 0.386  \n",
       "recall                                                0.606014  \n",
       "precision                                             0.628983  \n",
       "f1                                                    0.603586  \n",
       "predictions  [set_context, now, mkdir, epoch, add_volume, g...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table with result\n",
    "# Suprsingly the QLora does really good job, considering the fact that it's 4bit qunatized\n",
    "# For produciton it makes the most sense\n",
    "import pandas as pd\n",
    "\n",
    "results = {\n",
    "    'Base': results_base,\n",
    "    'LoRA': results_lora,\n",
    "    'Q-LoRA': results_qlora\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 511\n",
      "Expected: write_local_schema_file\n",
      "Predicted: get_files\n",
      "Source code: \n",
      "def x(self, cursor):\n",
      "    schema = []\n",
      "    tmp_schema_file_handle = NamedTemporaryFile(delete=True)\n",
      "    for name, type in zip(cursor.column_names, cursor.column_types):\n",
      "        schema.append(self.generate_schema_dict(name, type))\n",
      "    json_serialized_schema = json.dumps(schema).encode('utf-8')\n",
      "    tmp_schema_file_handle.write(json_serialized_schema)\n",
      "    return {self.schema_filename: tmp_schema_file_handle}\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 103\n",
      "Expected: get_conn\n",
      "Predicted: get_client\n",
      "Source code: \n",
      "def x(self):\n",
      "    http_authorized = self._authorize()\n",
      "    return build('dataproc', self.api_version, http=http_authorized,\n",
      "        cache_discovery=False)\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 314\n",
      "Expected: get_conn\n",
      "Predicted: get_connection\n",
      "Source code: \n",
      "def x(self):\n",
      "    service = self.get_service()\n",
      "    project = self._get_field('project')\n",
      "    return BigQueryConnection(service=service, project_id=project,\n",
      "        use_legacy_sql=self.use_legacy_sql, location=self.location,\n",
      "        num_retries=self.num_retries)\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 24\n",
      "Expected: bulk_load\n",
      "Predicted: copy_from\n",
      "Source code: \n",
      "def x(self, table, tmp_file):\n",
      "    self.copy_expert('COPY {table} FROM STDIN'.format(table=table), tmp_file)\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 377\n",
      "Expected: get_records\n",
      "Predicted: query\n",
      "Source code: \n",
      "def x(self, sql, parameters=None):\n",
      "    with closing(self.get_conn()) as conn:\n",
      "        with closing(conn.cursor()) as cur:\n",
      "            if parameters is not None:\n",
      "                cur.execute(sql, parameters)\n",
      "            else:\n",
      "                cur.execute(sql)\n",
      "            return cur.fetchall()\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 126\n",
      "Expected: load_string\n",
      "Predicted: put_string\n",
      "Source code: \n",
      "def x(self, string_data, container_name, blob_name, **kwargs):\n",
      "    self.connection.create_blob_from_text(container_name, blob_name,\n",
      "        string_data, **kwargs)\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 93\n",
      "Expected: resume_transfer_operation\n",
      "Predicted: resume_operation\n",
      "Source code: \n",
      "def x(self, operation_name):\n",
      "    self.get_conn().transferOperations().resume(name=operation_name).execute(\n",
      "        num_retries=self.num_retries)\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 316\n",
      "Expected: on_pre_execution\n",
      "Predicted: run_pre_exec_callbacks\n",
      "Source code: \n",
      "def x(**kwargs):\n",
      "    logging.debug('Calling callbacks: %s', __pre_exec_callbacks)\n",
      "    for cb in __pre_exec_callbacks:\n",
      "        try:\n",
      "            cb(**kwargs)\n",
      "        except Exception:\n",
      "            logging.exception('Failed on pre-execution callback using %s', cb)\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 95\n",
      "Expected: gcs_read\n",
      "Predicted: get_log_from_gcs\n",
      "Source code: \n",
      "def x(self, remote_log_location):\n",
      "    bkt, blob = self.parse_gcs_url(remote_log_location)\n",
      "    return self.hook.download(bkt, blob).decode('utf-8')\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 888\n",
      "Expected: get_one\n",
      "Predicted: read_value\n",
      "Source code: \n",
      "def x(cls, execution_date, key=None, task_id=None, dag_id=None,\n",
      "    include_prior_dates=False, session=None):\n",
      "    filters = []\n",
      "    if key:\n",
      "        filters.append(cls.key == key)\n",
      "    if task_id:\n",
      "        filters.append(cls.task_id == task_id)\n",
      "    if dag_id:\n",
      "        filters.append(cls.dag_id == dag_id)\n",
      "    if include_prior_dates:\n",
      "        filters.append(cls.execution_date <= execution_date)\n",
      "    else:\n",
      "        filters.append(cls.execution_date == execution_date)\n",
      "    query = session.query(cls.value).filter(and_(*filters)).order_by(cls.\n",
      "        execution_date.desc(), cls.timestamp.desc())\n",
      "    result = query.first()\n",
      "    if result:\n",
      "        enable_pickling = configuration.getboolean('core',\n",
      "            'enable_xcom_pickling')\n",
      "        if enable_pickling:\n",
      "            return pickle.loads(result.value)\n",
      "        else:\n",
      "            try:\n",
      "                return json.loads(result.value.decode('UTF-8'))\n",
      "            except ValueError:\n",
      "                log = LoggingMixin().log\n",
      "                log.error(\n",
      "                    'Could not deserialize the XCOM value from JSON. If you are using pickles instead of JSON for XCOM, then you need to enable pickle support for XCOM in your airflow config.'\n",
      "                    )\n",
      "                raise\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We will now continue with QLora results\n",
    "# First let's see where it made mistake\n",
    "import random\n",
    "\n",
    "expected = dataset[\"function_name\"]\n",
    "predicted = results_qlora[\"predictions\"]\n",
    "errors = [i for i in range(len(expected)) if expected[i] != predicted[i]]\n",
    "\n",
    "for i in random.sample(errors, 10):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Expected: {expected[i]}\")\n",
    "    print(f\"Predicted: {predicted[i]}\")\n",
    "    print(f\"Source code: \\n{dataset['source_code'][i]}\")\n",
    "    print(\"-\"*50)\n",
    "# I would say that the errors made by LLM are fairly reasonable.\n",
    "# Sometimes just a parts are mistakes as can be seen in realtively high f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of expected function names: 13.94\n",
      "Average length of predicted function names: 13.344\n"
     ]
    }
   ],
   "source": [
    "# Let's compare average length\n",
    "avg_len_expected = sum(len(name) for name in expected) / len(expected)\n",
    "avg_len_predicted = sum(len(name) for name in predicted) / len(predicted)\n",
    "\n",
    "print(f\"Average length of expected function names: {avg_len_expected}\")\n",
    "print(f\"Average length of predicted function names: {avg_len_predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after stemming: 0.386\n",
      "Example 833\n",
      "Expected: gzipped\n",
      "Predicted: gzip\n",
      "Source code: \n",
      "def x(f):\n",
      "\n",
      "    @functools.wraps(f)\n",
      "    def x(*args, **kwargs):\n",
      "\n",
      "        @after_this_request\n",
      "        def x(response):\n",
      "            accept_encoding = request.headers.get('Accept-Encoding', '')\n",
      "            if 'gzip' not in accept_encoding.lower():\n",
      "                return response\n",
      "            response.direct_passthrough = False\n",
      "            if (response.status_code < 200 or response.status_code >= 300 or\n",
      "                'Content-Encoding' in response.headers):\n",
      "                return response\n",
      "            gzip_buffer = IO()\n",
      "            gzip_file = gzip.GzipFile(mode='wb', fileobj=gzip_buffer)\n",
      "            gzip_file.write(response.data)\n",
      "            gzip_file.close()\n",
      "            response.data = gzip_buffer.getvalue()\n",
      "            response.headers['Content-Encoding'] = 'gzip'\n",
      "            response.headers['Vary'] = 'Accept-Encoding'\n",
      "            response.headers['Content-Length'] = len(response.data)\n",
      "            return response\n",
      "        return f(*args, **kwargs)\n",
      "    return view_func\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 204\n",
      "Expected: chunks\n",
      "Predicted: chunk\n",
      "Source code: \n",
      "def x(items, chunk_size):\n",
      "    if chunk_size <= 0:\n",
      "        raise ValueError('Chunk size must be a positive integer')\n",
      "    for i in range(0, len(items), chunk_size):\n",
      "        yield items[i:i + chunk_size]\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 399\n",
      "Expected: get_cgroup_names\n",
      "Predicted: get_cgroup_name\n",
      "Source code: \n",
      "def x():\n",
      "    with open('/proc/self/cgroup') as f:\n",
      "        lines = f.readlines()\n",
      "        d = {}\n",
      "        for line in lines:\n",
      "            line_split = line.rstrip().split(':')\n",
      "            subsystem = line_split[1]\n",
      "            group_name = line_split[2]\n",
      "            d[subsystem] = group_name\n",
      "        return d\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 724\n",
      "Expected: get_template_field\n",
      "Predicted: get_template_fields\n",
      "Source code: \n",
      "def x(env, fullname):\n",
      "    modname, classname = fullname.rsplit('.', 1)\n",
      "    try:\n",
      "        with mock(env.config.autodoc_mock_imports):\n",
      "            mod = import_module(modname)\n",
      "    except ImportError:\n",
      "        raise RoleException('Error loading %s module.' % (modname,))\n",
      "    clazz = getattr(mod, classname)\n",
      "    if not clazz:\n",
      "        raise RoleException('Error finding %s class in %s module.' % (\n",
      "            classname, modname))\n",
      "    template_fields = getattr(clazz, 'template_fields')\n",
      "    if not template_fields:\n",
      "        raise RoleException(\n",
      "            'Could not find the template fields for %s class in %s module.' %\n",
      "            (classname, modname))\n",
      "    return list(template_fields)\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 344\n",
      "Expected: call_and_grads\n",
      "Predicted: call_and_grad\n",
      "Source code: \n",
      "def x(fn: TransitionOperator, args: Union[Tuple[Any], Any]) ->Tuple[tf.\n",
      "    Tensor, TensorNest, TensorNest]:\n",
      "    with tf.GradientTape() as tape:\n",
      "        tape.watch(args)\n",
      "        ret, extra = call_fn(fn, args)\n",
      "    grads = tape.gradient(ret, args)\n",
      "    return ret, extra, grads\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 958\n",
      "Expected: get_init_containers\n",
      "Predicted: get_init_container\n",
      "Source code: \n",
      "def x(self):\n",
      "    if (self.kube_config.dags_volume_claim or self.kube_config.\n",
      "        dags_volume_host or self.kube_config.dags_in_image):\n",
      "        return []\n",
      "    init_environment = [{'name': 'GIT_SYNC_REPO', 'value': self.kube_config\n",
      "        .git_repo}, {'name': 'GIT_SYNC_BRANCH', 'value': self.kube_config.\n",
      "        git_branch}, {'name': 'GIT_SYNC_ROOT', 'value': self.kube_config.\n",
      "        git_sync_root}, {'name': 'GIT_SYNC_DEST', 'value': self.kube_config\n",
      "        .git_sync_dest}, {'name': 'GIT_SYNC_DEPTH', 'value': '1'}, {'name':\n",
      "        'GIT_SYNC_ONE_TIME', 'value': 'true'}]\n",
      "    if self.kube_config.git_user:\n",
      "        init_environment.append({'name': 'GIT_SYNC_USERNAME', 'value': self\n",
      "            .kube_config.git_user})\n",
      "    if self.kube_config.git_password:\n",
      "        init_environment.append({'name': 'GIT_SYNC_PASSWORD', 'value': self\n",
      "            .kube_config.git_password})\n",
      "    volume_mounts = [{'mountPath': self.kube_config.git_sync_root, 'name':\n",
      "        self.dags_volume_name, 'readOnly': False}]\n",
      "    if self.kube_config.git_ssh_key_secret_name:\n",
      "        volume_mounts.append({'name': self.git_sync_ssh_secret_volume_name,\n",
      "            'mountPath': '/etc/git-secret/ssh', 'subPath': 'ssh'})\n",
      "        init_environment.extend([{'name': 'GIT_SSH_KEY_FILE', 'value':\n",
      "            '/etc/git-secret/ssh'}, {'name': 'GIT_SYNC_SSH', 'value': 'true'}])\n",
      "    if self.kube_config.git_ssh_known_hosts_configmap_name:\n",
      "        volume_mounts.append({'name': self.\n",
      "            git_sync_ssh_known_hosts_volume_name, 'mountPath':\n",
      "            '/etc/git-secret/known_hosts', 'subPath': 'known_hosts'})\n",
      "        init_environment.extend([{'name': 'GIT_KNOWN_HOSTS', 'value':\n",
      "            'true'}, {'name': 'GIT_SSH_KNOWN_HOSTS_FILE', 'value':\n",
      "            '/etc/git-secret/known_hosts'}])\n",
      "    else:\n",
      "        init_environment.append({'name': 'GIT_KNOWN_HOSTS', 'value': 'false'})\n",
      "    return [{'name': self.kube_config.git_sync_init_container_name, 'image':\n",
      "        self.kube_config.git_sync_container, 'securityContext': {\n",
      "        'runAsUser': 65533}, 'env': init_environment, 'volumeMounts':\n",
      "        volume_mounts}]\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 575\n",
      "Expected: list_transfer_job\n",
      "Predicted: list_transfer_jobs\n",
      "Source code: \n",
      "def x(self, filter):\n",
      "    conn = self.get_conn()\n",
      "    filter = self._inject_project_id(filter, FILTER, FILTER_PROJECT_ID)\n",
      "    request = conn.transferJobs().list(filter=json.dumps(filter))\n",
      "    jobs = []\n",
      "    while request is not None:\n",
      "        response = request.execute(num_retries=self.num_retries)\n",
      "        jobs.extend(response[TRANSFER_JOBS])\n",
      "        request = conn.transferJobs().list_next(previous_request=request,\n",
      "            previous_response=response)\n",
      "    return jobs\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lastly let's see how much will accuracy rise if we use stemming\n",
    "# Unfornutately almost not at all only 7 sampels were fixed\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_expected = [\"_\".join(stemmer.stem(token) for token in get_fc_name_tokens(name)) for name in expected]\n",
    "stemmed_predicted = [\"_\".join(stemmer.stem(token) for token in get_fc_name_tokens(name)) for name in predicted]\n",
    "errors_stemmed = [i for i in range(len(stemmed_expected)) if stemmed_expected[i] != stemmed_predicted[i]]\n",
    "\n",
    "print(f\"Accuracy after stemming: {1 - len(errors)/len(expected)}\")\n",
    "\n",
    "# Let's see which erros were fixed\n",
    "fixed_indices = set(errors) - set(errors_stemmed)\n",
    "for i in fixed_indices:\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Expected: {expected[i]}\")\n",
    "    print(f\"Predicted: {predicted[i]}\")\n",
    "    print(f\"Source code: \\n{dataset['source_code'][i]}\")\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
