
  0%|                                                                                                                                                                        | 0/3 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-12-27 02:11:47,999 >> You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 1.2727, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.33}
{'eval_loss': 1.2840009927749634, 'eval_runtime': 0.5018, 'eval_samples_per_second': 19.93, 'eval_steps_per_second': 5.979, 'epoch': 0.33}
{'train_runtime': 4.5773, 'train_samples_per_second': 2.185, 'train_steps_per_second': 0.655, 'train_loss': 1.2727434635162354, 'epoch': 0.33}
***** train metrics *****
  epoch                    =       0.33
  train_loss               =     1.2727
  train_runtime            = 0:00:04.57
  train_samples            =         10
  train_samples_per_second =      2.185
  train_steps_per_second   =      0.655
2023-12-27 02:11:50 - INFO - __main__ - *** Evaluate ***
***** eval metrics *****
  epoch                   =       0.33
  eval_loss               =      1.284
  eval_runtime            = 0:00:00.50
  eval_samples            =         10
  eval_samples_per_second =     19.941
  eval_steps_per_second   =      5.982
2023-12-27 02:11:51 - INFO - __main__ - *** Save model ***
 33%|█████████████████████████████████████████████████████▎                                                                                                          | 1/3 [00:02<00:04,  2.09s/it][INFO|trainer.py:3158] 2023-12-27 02:11:50,081 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2023-12-27 02:11:50,081 >>   Num examples = 10
[INFO|trainer.py:3163] 2023-12-27 02:11:50,081 >>   Batch size = 4
 33%|█████████████████████████████████████████████████████▎                                                                                                          | 1/3 [00:02<00:04,  2.09s/it][INFO|trainer.py:1955] 2023-12-27 02:11:50,584 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
 33%|█████████████████████████████████████████████████████▎                                                                                                          | 1/3 [00:02<00:05,  2.59s/it]
[INFO|trainer.py:3158] 2023-12-27 02:11:50,587 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2023-12-27 02:11:50,587 >>   Num examples = 10
[INFO|trainer.py:3163] 2023-12-27 02:11:50,587 >>   Batch size = 4
 33%|█████████████████████████████████████████████████████▎                                                                                                          | 1/3 [00:00<00:00,  4.98it/s]
[INFO|trainer.py:2881] 2023-12-27 02:11:51,091 >> Saving model checkpoint to /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names
[INFO|tokenization_utils_base.py:2428] 2023-12-27 02:11:51,499 >> tokenizer config file saved in /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-12-27 02:11:51,500 >> Special tokens file saved in /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names/special_tokens_map.json
[INFO|trainer.py:2881] 2023-12-27 02:11:51,527 >> Saving model checkpoint to /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names
[INFO|tokenization_utils_base.py:2428] 2023-12-27 02:11:52,115 >> tokenizer config file saved in /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-12-27 02:11:52,116 >> Special tokens file saved in /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names/special_tokens_map.json
training_args.bin: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.73k/4.73k [00:00<00:00, 14.2kB/s]
adapter_model.safetensors:   1%|█▋                                                                                                                             | 7.01M/537M [00:00<00:44, 11.9MB/s]
Upload 2 LFS files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:51<00:00, 25.70s/it]
[INFO|modelcard.py:452] 2023-12-27 02:12:45,941 >> Dropping the following result as it does not have all the necessary fields:                                       | 1/2 [00:51<00:51, 51.40s/it]
{'dataset': {'name': '/workspace/s3/hf/datasets/python_code_cleaned', 'type': '/workspace/s3/hf/datasets/python_code_cleaned'}}
[INFO|configuration_utils.py:461] 2023-12-27 02:12:45,943 >> Configuration saved in /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names/config.json
[INFO|trainer.py:2881] 2023-12-27 02:12:45,943 >> Saving model checkpoint to /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names
2023-12-27 02:12:45 - INFO - __main__ - Model saved to /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names
2023-12-27 02:12:45 - INFO - __main__ - Pushing to hub...
[INFO|tokenization_utils_base.py:2428] 2023-12-27 02:12:46,534 >> tokenizer config file saved in /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-12-27 02:12:46,534 >> Special tokens file saved in /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names/special_tokens_map.json