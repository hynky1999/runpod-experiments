  0%|                                                                                                                                                                        | 0/2 [00:00<?, ?it/s]/workspace/venv/lib/python3.10/site-packages/trl/trainer/utils.py:570: UserWarning: The dataset reached end and the iterator is reset to the start.
  warnings.warn("The dataset reached end and the iterator is reset to the start.")
[WARNING|logging.py:314] 2023-12-27 03:46:10,699 >> You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[2023-12-27 03:46:10,782] [0/0] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
 50%|████████████████████████████████████████████████████████████████████████████████                                                                                | 1/2 [00:30<00:30, 30.65s/it][INFO|trainer.py:2881] 2023-12-27 03:46:39,488 >> Saving model checkpoint to /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names/checkpoint-1
[INFO|tokenization_utils_base.py:2428] 2023-12-27 03:46:40,081 >> tokenizer config file saved in /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names/checkpoint-1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-12-27 03:46:40,081 >> Special tokens file saved in /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names/checkpoint-1/special_tokens_map.json
{'loss': 1.5394, 'learning_rate': 4e-05, 'epoch': 0.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:56<00:00, 27.85s/it][INFO|trainer.py:2881] 2023-12-27 03:47:05,380 >> Saving model checkpoint to /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names/checkpoint-2
[INFO|tokenization_utils_base.py:2428] 2023-12-27 03:47:05,988 >> tokenizer config file saved in /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names/checkpoint-2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-12-27 03:47:05,988 >> Special tokens file saved in /workspace/s3/hf/experiments/codellama-7b-sft-lora-func-names/checkpoint-2/special_tokens_map.json
[INFO|trainer.py:3158] 2023-12-27 03:47:07,626 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2023-12-27 03:47:07,626 >>   Num examples = 2000
[INFO|trainer.py:3163] 2023-12-27 03:47:07,626 >>   Batch size = 1


  File "/workspace/runpod-experiments/scripts/run_sft.py", line 203, in <module>                                                                                 | 13/2000 [00:06<19:19,  1.71it/s]
    main()
  File "/workspace/runpod-experiments/scripts/run_sft.py", line 156, in main
    train_result = trainer.train()
  File "/workspace/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 280, in train
    output = super().train(*args, **kwargs)
  File "/workspace/venv/lib/python3.10/site-packages/transformers/trainer.py", line 1546, in train
    return inner_training_loop(
  File "/workspace/venv/lib/python3.10/site-packages/transformers/trainer.py", line 1937, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/workspace/venv/lib/python3.10/site-packages/transformers/trainer.py", line 2271, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/workspace/venv/lib/python3.10/site-packages/transformers/trainer.py", line 3011, in evaluate
    output = eval_loop(
  File "/workspace/venv/lib/python3.10/site-packages/transformers/trainer.py", line 3226, in evaluation_loop
    preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)
  File "/workspace/venv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py", line 123, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/workspace/venv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py", line 82, in torch_pad_and_concatenate
    return torch.cat((tensor1, tensor2), dim=0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.42 GiB. GPU 0 has a total capacty of 23.69 GiB of which 3.28 GiB is free. Process 359354 has 20.41 GiB memory in use. Of the allocated memory 18.24 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF