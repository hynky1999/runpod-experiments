
  0%|                                                                                                                                                                       | 0/31 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-12-27 03:07:42,631 >> You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 1.4447, 'learning_rate': 1e-05, 'epoch': 0.03}


 10%|███████████████▍                                                                                                                                               | 3/31 [01:15<11:29, 24.63s/it]
{'train_runtime': 88.9633, 'train_samples_per_second': 5.62, 'train_steps_per_second': 0.348, 'train_loss': 1.697432557741801, 'epoch': 0.1}
***** train metrics *****
  epoch                    =        0.1
  train_loss               =     1.6974
  train_runtime            = 0:01:28.96
  train_samples            =        500
  train_samples_per_second =       5.62
  train_steps_per_second   =      0.348
 10%|███████████████▍                                                                                                                                               | 3/31 [01:15<11:29, 24.63s/it][INFO|trainer.py:1955] 2023-12-27 03:09:09,796 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
 10%|███████████████▍                                                                                                                                               | 3/31 [01:27<13:34, 29.09s/it]
[INFO|trainer.py:3158] 2023-12-27 03:09:09,800 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2023-12-27 03:09:09,800 >>   Num examples = 500
[INFO|trainer.py:3163] 2023-12-27 03:09:09,800 >>   Batch size = 1



  3%|████▋                                                                                                                                                        | 15/500 [00:07<04:51,  1.66it/s]Traceback (most recent call last):
  File "/workspace/runpod-experiments/scripts/run_sft.py", line 206, in <module>
    main()
  File "/workspace/runpod-experiments/scripts/run_sft.py", line 172, in main
    metrics = trainer.evaluate()
  File "/workspace/venv/lib/python3.10/site-packages/transformers/trainer.py", line 3011, in evaluate
    output = eval_loop(
  File "/workspace/venv/lib/python3.10/site-packages/transformers/trainer.py", line 3226, in evaluation_loop
    preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)
  File "/workspace/venv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py", line 123, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/workspace/venv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py", line 82, in torch_pad_and_concatenate
    return torch.cat((tensor1, tensor2), dim=0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.91 GiB. GPU 0 has a total capacty of 23.69 GiB of which 3.49 GiB is free. Process 310853 has 20.19 GiB memory in use. Of the allocated memory 18.73 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF