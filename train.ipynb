{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'axolotl'...\n",
      "remote: Enumerating objects: 8455, done.\u001b[K\n",
      "remote: Counting objects: 100% (4040/4040), done.\u001b[K\n",
      "remote: Compressing objects: 100% (968/968), done.\u001b[K\n",
      "remote: Total 8455 (delta 3368), reused 3112 (delta 3065), pack-reused 4415\u001b[K\n",
      "Receiving objects: 100% (8455/8455), 2.77 MiB | 16.81 MiB/s, done.\n",
      "Resolving deltas: 100% (5533/5533), done.\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (23.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Obtaining file:///workspace/runpod-experiments/llama2-function-name-prediction\n",
      "\u001b[31mERROR: file:///workspace/runpod-experiments/llama2-function-name-prediction does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/OpenAccess-AI-Collective/axolotl\n",
    "!cd axolotl\n",
    "\n",
    "!pip3 install packaging\n",
    "!pip3 install -e '.[flash-attn,deepspeed]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd alignment-handbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Supervised fine-tuning script for decoder language models.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import random\n",
    "import sys\n",
    "import wandb\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import set_seed\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from alignment import (\n",
    "    DataArguments,\n",
    "    H4ArgumentParser,\n",
    "    ModelArguments,\n",
    "    SFTConfig,\n",
    "    apply_chat_template,\n",
    "    get_datasets,\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    "    get_tokenizer,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def main():\n",
    "    wandb.init(project=\"function\", entity=\"hynky\", name=\"sft\")\n",
    "    parser = H4ArgumentParser((ModelArguments, DataArguments, SFTConfig))\n",
    "    model_args, data_args, training_args = parser.parse()\n",
    "\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    log_level = training_args.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "    # Log on each process a small summary\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    logger.info(f\"Model parameters {model_args}\")\n",
    "    logger.info(f\"Data parameters {data_args}\")\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "    ###############\n",
    "    # Load datasets\n",
    "    ###############\n",
    "    raw_datasets = get_datasets(data_args, splits=data_args.dataset_splits)\n",
    "    logger.info(\n",
    "        f\"Training on the following datasets and their proportions: {[split + ' : ' + str(dset.num_rows) for split, dset in raw_datasets.items()]}\"\n",
    "    )\n",
    "\n",
    "    ################\n",
    "    # Load tokenizer\n",
    "    ################\n",
    "    tokenizer = get_tokenizer(model_args, data_args)\n",
    "\n",
    "    #####################\n",
    "    # Apply chat template\n",
    "    #####################\n",
    "    raw_datasets = raw_datasets.map(apply_chat_template, fn_kwargs={\"tokenizer\": tokenizer, \"task\": \"sft\"})\n",
    "    train_dataset = raw_datasets[\"train\"]\n",
    "    eval_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "    with training_args.main_process_first(desc=\"Log a few random samples from the processed training set\"):\n",
    "        for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
    "            logger.info(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")\n",
    "\n",
    "    #######################\n",
    "    # Load pretrained model\n",
    "    #######################\n",
    "    logger.info(\"*** Load pretrained model ***\")\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    quantization_config = get_quantization_config(model_args)\n",
    "\n",
    "    model_kwargs = dict(\n",
    "        revision=model_args.model_revision,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        use_flash_attention_2=model_args.use_flash_attention_2,\n",
    "        torch_dtype=torch_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,\n",
    "        device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "    logger.info(\"*** Model loaded! ***\")\n",
    "\n",
    "    ########################\n",
    "    # Initialize the Trainer\n",
    "    ########################\n",
    "    trainer = SFTTrainer(\n",
    "        model=model_args.model_name_or_path,\n",
    "        model_init_kwargs=model_kwargs,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=training_args.max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        peft_config=get_peft_config(model_args),\n",
    "    )\n",
    "\n",
    "    ###############\n",
    "    # Training loop\n",
    "    ###############\n",
    "    logger.info(\"*** Train ***\")\n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    ##########\n",
    "    # Evaluate\n",
    "    ##########\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "        metrics = trainer.evaluate()\n",
    "        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
    "        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "        trainer.log_metrics(\"eval\", metrics)\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "    ##################################\n",
    "    # Save model and create model card\n",
    "    ##################################\n",
    "    logger.info(\"*** Save model ***\")\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "    logger.info(f\"Model saved to {training_args.output_dir}\")\n",
    "\n",
    "    # Save everything else on main process\n",
    "    if accelerator.is_main_process:\n",
    "        kwargs = {\n",
    "            \"finetuned_from\": model_args.model_name_or_path,\n",
    "            \"dataset\": list(data_args.dataset_mixer.keys()),\n",
    "            \"dataset_tags\": list(data_args.dataset_mixer.keys()),\n",
    "            \"tags\": [\"alignment-handbook\"],\n",
    "        }\n",
    "        trainer.create_model_card(**kwargs)\n",
    "        # Restore k,v cache for fast inference\n",
    "        trainer.model.config.use_cache = True\n",
    "        trainer.model.config.save_pretrained(training_args.output_dir)\n",
    "\n",
    "        if training_args.push_to_hub is True:\n",
    "            logger.info(\"Pushing to hub...\")\n",
    "            trainer.push_to_hub()\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
